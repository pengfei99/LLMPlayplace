{"cells":[{"cell_type":"markdown","metadata":{"id":"aTaDCGTe78bK"},"source":["# Fine-tune Llama 3.1 8B with Unsloth"]},{"cell_type":"markdown","metadata":{},"source":["[Unsloth AI](https://unsloth.ai) est une startup qui propose un package Python, `unsloth`, pour fine-tuner en local son LLM. Ils supportent notamment Llama-3 de Meta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoPKQjga6obN"},"outputs":[],"source":["!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n","from torch import __version__; from packaging.version import Version as V\n","xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n","!pip install -qqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from trl import SFTTrainer\n","from datasets import load_dataset\n","from transformers import TrainingArguments, TextStreamer\n","from unsloth.chat_templates import get_chat_template\n","from unsloth import FastLanguageModel, is_bfloat16_supported"]},{"cell_type":"markdown","metadata":{"id":"matKaF-f-GiU"},"source":["## 1. Load model for PEFT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGX9wG7Lhc-z"},"outputs":[],"source":["# Load pre-trained Llama 3.1 8B model\n","max_seq_length = 2048\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n","    max_seq_length=max_seq_length,\n","    load_in_4bit=True,\n","    dtype=None,\n",")\n","\n","# Prepare model for PEFT\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r=16,  # LoRA matrix rank\n","    lora_alpha=16,\n","    lora_dropout=0,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], # modules on which to apply LoRA\n","    use_rslora=True,\n","    use_gradient_checkpointing=\"unsloth\"\n",")\n","print(model.print_trainable_parameters())"]},{"cell_type":"markdown","metadata":{"id":"hjDpwfjJ3RAL"},"source":["## 2. Prepare data and tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqGnvaT8is-R"},"outputs":[],"source":["# Format chat messages for input to model\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template=\"chatml\",\n","    mapping={\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}\n",")\n","\n","# Apply chat formatting to dataset\n","def apply_template(examples):\n","    messages = examples[\"conversations\"]\n","    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n","    return {\"text\": text}\n","\n","# Load fine-tuning dataset (see https://huggingface.co/datasets/mlabonne/FineTome-100k)\n","dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n","# Apply dataset formatting in batch mode\n","dataset = dataset.map(apply_template, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"zdfjufQd3XMi"},"source":["## 3. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcPAQihcjcfl"},"outputs":[],"source":["# Set torch device\n","torch.cuda.set_device(1)\n","\n","# Define training arguments\n","args = TrainingArguments(\n","    learning_rate=3e-4,\n","    lr_scheduler_type=\"linear\",\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    num_train_epochs=1,\n","    fp16=not is_bfloat16_supported(),\n","    bf16=is_bfloat16_supported(),\n","    logging_steps=1,\n","    optim=\"adamw_8bit\",\n","    weight_decay=0.01,\n","    warmup_steps=10,\n","    output_dir=\"output\",\n","    seed=0,\n",")\n","print(args.device)\n","\n","# Define trainer module (SFT = Supervised Fine-Tuning)\n","trainer=SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    dataset_num_proc=2,\n","    packing=True,\n","    args=args,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train model\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"CI_U9FHZ3ZLO"},"source":["## 4. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JXdjsLqkvZY"},"outputs":[],"source":["# Load model for inference\n","model = FastLanguageModel.for_inference(model)\n","\n","messages = [\n","    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=True,\n","    add_generation_prompt=True,\n","    return_tensors=\"pt\",\n",").to(\"cuda\")\n","\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z","timestamp":1728502626113},{"file_id":"16GkVYWq4CTmUzoL6-LR9hQloLpbaxCj-","timestamp":1722072326388},{"file_id":"1_e9wNG2hv4PjBfzVtg8hYmU8gZGgPL2u","timestamp":1721815509495}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
