{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required libraries\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "import chromadb\n",
    "import os"
   ],
   "id": "c79e70b9fdb157f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the LLM model to be used\n",
    "llm_model = \"llama3.2\"\n",
    "\n",
    "# Configure ChromaDB\n",
    "# Initialize the ChromaDB client with persistent storage in the current directory\n",
    "chroma_client = chromadb.PersistentClient(path=os.path.join(os.getcwd(), \"chroma_db\"))\n",
    "\n"
   ],
   "id": "3255c8e1e3d673bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Define a custom embedding function for ChromaDB using Ollama\n",
    "class ChromaDBEmbeddingFunction:\n",
    "    \"\"\"\n",
    "    Custom embedding function for ChromaDB using embeddings from Ollama.\n",
    "    \"\"\"\n",
    "    def __init__(self, langchain_embeddings):\n",
    "        self.langchain_embeddings = langchain_embeddings\n",
    "\n",
    "    def __call__(self, input):\n",
    "        # Ensure the input is in a list format for processing\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        return self.langchain_embeddings.embed_documents(input)\n",
    "\n",
    "# Initialize the embedding function with Ollama embeddings\n",
    "embedding = ChromaDBEmbeddingFunction(\n",
    "    OllamaEmbeddings(\n",
    "        model=llm_model,\n",
    "        base_url=\"http://localhost:11434\"  # Adjust the base URL as per your Ollama server configuration\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define a collection for the RAG workflow\n",
    "collection_name = \"rag_collection_demo_1\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"A collection for RAG with Ollama - Demo1\"},\n",
    "    embedding_function=embedding  # Use the custom embedding function\n",
    ")\n",
    "\n",
    "# Function to add documents to the ChromaDB collection\n",
    "def add_documents_to_collection(documents:List[str], ids:List[str]):\n",
    "   \"\"\"\n",
    "   This function takes two list of strings and adds them to the ChromaDB collection.\n",
    "   :param documents: \n",
    "   :param ids: \n",
    "   :return: \n",
    "   \"\"\"\n",
    "   collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "   )\n",
    "\n",
    "# Example: Add sample documents to the collection\n",
    "documents = [\n",
    "    \"Artificial intelligence is the simulation of human intelligence processes by machines.\",\n",
    "    \"Python is a programming language that lets you work quickly and integrate systems more effectively.\",\n",
    "    \"ChromaDB is a vector database designed for AI applications.\"\n",
    "]\n",
    "doc_ids = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "\n",
    "# Documents only need to be added once or whenever an update is required. \n",
    "# This line of code is included for demonstration purposes:\n",
    "add_documents_to_collection(documents, doc_ids)\n",
    "\n"
   ],
   "id": "655a11add6f9f4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to query the ChromaDB collection\n",
    "def query_chromadb(query_text, n_results=1):\n",
    "    \"\"\"\n",
    "    Query the ChromaDB collection for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The input query.\n",
    "        n_results (int): The number of top results to return.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: The top matching documents and their metadata.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results[\"documents\"], results[\"metadatas\"]\n",
    "\n",
    "# Function to interact with the Ollama LLM\n",
    "def query_ollama(prompt):\n",
    "    \"\"\"\n",
    "    Send a query to Ollama and retrieve the response.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt for Ollama.\n",
    "    \n",
    "    Returns:\n",
    "        str: The response from Ollama.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=llm_model)\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "# RAG pipeline: Combine ChromaDB and Ollama for Retrieval-Augmented Generation\n",
    "def rag_pipeline(query_text):\n",
    "    \"\"\"\n",
    "    Perform Retrieval-Augmented Generation (RAG) by combining ChromaDB and Ollama.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The input query.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response from Ollama augmented with retrieved context.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents from ChromaDB\n",
    "    retrieved_docs, metadata = query_chromadb(query_text)\n",
    "    context = \" \".join(retrieved_docs[0]) if retrieved_docs else \"No relevant documents found.\"\n",
    "\n",
    "    # Step 2: Send the query along with the context to Ollama\n",
    "    augmented_prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "    print(\"######## Augmented Prompt ########\")\n",
    "    print(augmented_prompt)\n",
    "\n",
    "    response = query_ollama(augmented_prompt)\n",
    "    return response\n",
    "\n"
   ],
   "id": "814a65ebfcd4dd3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage\n",
    "# Define a query to test the RAG pipeline\n",
    "query = \"What is artificial intelligence?\"  # Change the query as needed\n",
    "response = rag_pipeline(query)\n",
    "print(\"######## Response from LLM ########\\n\", response)"
   ],
   "id": "184fc35520eabf9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
