{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I will use the langchain ollama integration ()\n"
   ],
   "id": "911be57eebd790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:14.004618Z",
     "start_time": "2025-01-20T12:56:11.545431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required libraries\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from typing import List"
   ],
   "id": "c79e70b9fdb157f2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:14.029415Z",
     "start_time": "2025-01-20T12:56:14.017360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the LLM model to be used\n",
    "llm_model = \"steamdj/llama3.1-cpu-only\"\n",
    "print(f\"embedding model name: {llm_model}\")\n",
    "\n",
    "# define chroma db storage path\n",
    "current_dir:Path = Path.cwd()\n",
    "chroma_db_path:Path = current_dir.joinpath(\"vector_db\")\n",
    "\n",
    "# define the max context documents which the vector db can return\n",
    "MAX_CONTEXT_DOC_SIZE:int = 5\n",
    "\n",
    "print(f\"vector db path: {chroma_db_path}\")"
   ],
   "id": "3255c8e1e3d673bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding model name: steamdj/llama3.1-cpu-only\n",
      "vector db path: /home/pliu/git/LLMPlayplace/ollama/02_advance/ollama_rag/vector_db\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step1. Prepare indexing system\n",
    "\n",
    "In this step, we prepare an indexing system which contains two parts:\n",
    "  - An embedding model: which can transfer documents into vectors.\n",
    "  - A vector DB: which stores the embeddings (The list of the vectors)\n",
    "\n",
    "In this tutorial\n",
    "- We use `langchain_ollama` api to connect to an ollama model runtime to implement the embedding model.\n",
    "- We use `ChromaDB` as the vector db.\n",
    "\n",
    "\n",
    "### 1.1 Building the embedding model"
   ],
   "id": "15f4c967fc39c7ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:14.470951Z",
     "start_time": "2025-01-20T12:56:14.425420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create an embedding model which can transform the input document into vector.\n",
    "# In this example, we use the api of `langchain_ollama` which links to a local ollama model\n",
    "embedding_model = OllamaEmbeddings(\n",
    "        model=llm_model,\n",
    "        base_url=\"http://localhost:11434\"  # Adjust the base URL as per your Ollama server configuration\n",
    "    )\n",
    "\n",
    "print(type(embedding_model))"
   ],
   "id": "31e263a7cb84aa55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_ollama.embeddings.OllamaEmbeddings'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:14.591458Z",
     "start_time": "2025-01-20T12:56:14.583895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a custom embedding function for the vector db(ChromaDB)\n",
    "class EmbeddingFunction:\n",
    "    \"\"\"\n",
    "    Custom embedding function which takes an embedding model(compatible with ChromaDB).\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings_model: OllamaEmbeddings):\n",
    "        self.langchain_embeddings = embeddings_model\n",
    "\n",
    "    def __call__(self, input:List[str]):\n",
    "        # the signature of this function must be __call__(self, input). You can not modify the argument name\n",
    "        # Ensure the input is in a list format for processing\n",
    "        if isinstance(input, str):\n",
    "            input:List[str] = [input]\n",
    "        return self.langchain_embeddings.embed_documents(input)\n",
    "\n",
    "# Initialize the embedding function with Ollama embeddings\n",
    "embedding = EmbeddingFunction(embedding_model)"
   ],
   "id": "7810e8900d4eda8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Building the vector db",
   "id": "b1e42837c0017225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:17.150974Z",
     "start_time": "2025-01-20T12:56:16.701706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure ChromaDB\n",
    "# Initialize the ChromaDB client with persistent storage in the current directory\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_db_path.as_posix())\n",
    "\n",
    "# Define a collection for the RAG workflow\n",
    "collection_name = \"casd_doc_collection\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"A collection for RAG for casd\"},\n",
    "    embedding_function=embedding  # Use the custom embedding function\n",
    ")\n",
    "\n",
    "# Function to add documents to the ChromaDB collection\n",
    "def add_documents_to_collection(documents:List[str], ids:List[str]):\n",
    "   \"\"\"\n",
    "   This function takes two list of strings and adds them to the ChromaDB collection.\n",
    "   :param documents: A list of documents, each document represents the content of the document.\n",
    "   :param ids: A list of ids, each item is the id of the document.\n",
    "   :return: \n",
    "   \"\"\"\n",
    "   collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "   )\n"
   ],
   "id": "655a11add6f9f4c0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Prepare retrieval system\n",
    "\n",
    "In this step, we prepare a system which can retrive context based on the giving user query."
   ],
   "id": "8fa54ef339d673a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:18.429519Z",
     "start_time": "2025-01-20T12:56:18.421851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to query the ChromaDB collection\n",
    "def query_chromadb(query_text:str, n_results=MAX_CONTEXT_DOC_SIZE):\n",
    "    \"\"\"\n",
    "    The function takes a query text and returns a list of related documents in the vector db.\n",
    "    :param query_text: user input query text\n",
    "    :param n_results: The number of results to return\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results[\"documents\"], results[\"metadatas\"]\n",
    "\n"
   ],
   "id": "814a65ebfcd4dd3c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:19.214940Z",
     "start_time": "2025-01-20T12:56:19.208954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_compact_context(retrieved_docs:List[List[str]])->str:\n",
    "    \"\"\"\n",
    "    This function takes a list of retrieved documents and builds a compact context in a single string.\n",
    "    :param retrieved_docs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if retrieved_docs and len(retrieved_docs)>0:\n",
    "        return \";\".join(retrieved_docs[0])\n",
    "    else:\n",
    "        return \"No relevant documents found.\"\n"
   ],
   "id": "f158c72f23036bbe",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:28.699821Z",
     "start_time": "2025-01-20T12:56:20.062384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_doc = [\n",
    "    \"Pengfei loves to eat appels, Pengfei loves to eat meats\",\n",
    "    \"Pengfei playes lots of video games\",\n",
    "    \"Pengfei want to learn swimming\"\n",
    "]\n",
    "sample_doc_id = [\"1\",\"2\",\"3\"]\n",
    "\n",
    "add_documents_to_collection(sample_doc, sample_doc_id)\n"
   ],
   "id": "ccbabccafe3d5f65",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:31.568371Z",
     "start_time": "2025-01-20T12:56:28.724927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_query=\"What food does pengfei like the most?\"\n",
    "docs, metadatas = query_chromadb(sample_query)\n",
    "print(docs)\n",
    "print(metadatas)"
   ],
   "id": "9699f50389010025",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pengfei playes lots of video games', 'Pengfei loves to eat appels, Pengfei loves to eat meats', 'Pengfei want to learn swimming']]\n",
      "[[None, None, None]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:46.541948Z",
     "start_time": "2025-01-20T12:56:46.536381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to interact with the Ollama LLM\n",
    "def query_ollama(prompt):\n",
    "    \"\"\"\n",
    "    This function can send query to ollama llm model and retrive response.\n",
    "    :param prompt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=llm_model)\n",
    "    return llm.invoke(prompt)"
   ],
   "id": "f5bb46219d8bcd4a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:47.487788Z",
     "start_time": "2025-01-20T12:56:47.478775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG pipeline: Combine ChromaDB and Ollama for Retrieval-Augmented Generation\n",
    "def rag_pipeline(query_text: str):\n",
    "    \"\"\"\n",
    "    This function implements the RAG pipeline.\n",
    "    :param query_text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents from ChromaDB\n",
    "    retrieved_docs, metadata = query_chromadb(query_text)\n",
    "    context = build_compact_context(retrieved_docs)\n",
    "\n",
    "    # Step 2: Send the query along with the context to Ollama\n",
    "    augmented_prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "    print(\"######## Augmented Prompt ########\")\n",
    "    print(augmented_prompt)\n",
    "\n",
    "    response = query_ollama(augmented_prompt)\n",
    "    return response"
   ],
   "id": "7466993505a7d557",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:56:51.814061Z",
     "start_time": "2025-01-20T12:56:48.528511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: Add sample documents to the collection\n",
    "documents2 = [\n",
    "    \"Pengfei is a data scientist who works for CASD\",\n",
    "]\n",
    "doc_ids2 = [\"doc4\"]\n",
    "\n",
    "# Documents only need to be added once or whenever an update is required.\n",
    "# This line of code is included for demonstration purposes:\n",
    "add_documents_to_collection(documents2, doc_ids2)\n"
   ],
   "id": "dd7cda3d235f8743",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:58:51.280817Z",
     "start_time": "2025-01-20T12:56:51.825184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "# Define a query to test the RAG pipeline\n",
    "query1 = (\"who is Pengfei?\")  # Change the query as needed\n",
    "response1 = rag_pipeline(query1)\n",
    "print(\"######## Response from LLM ########\\n\", response1)"
   ],
   "id": "184fc35520eabf9f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Augmented Prompt ########\n",
      "Context: Pengfei playes lots of video games;Pengfei loves to eat appels, Pengfei loves to eat meats;Pengfei want to learn swimming;Pengfei is a data scientist who works for CASD\n",
      "\n",
      "Question: who is Pengfei?\n",
      "Answer:\n",
      "######## Response from LLM ########\n",
      " Based on the context provided, it appears that \"Pengfei\" refers to an individual with several characteristics and interests. Here's what can be inferred:\n",
      "\n",
      "1. **Gamer**: The person plays lots of video games.\n",
      "2. **Fruit lover**: They enjoy eating apples.\n",
      "3. **Meat enthusiast**: They have a preference for meats.\n",
      "4. **Aspiring swimmer**: There is an interest in learning how to swim.\n",
      "5. **Data scientist**: They work as a data scientist, specifically working at CASD (which stands for Computer Applications and Systems Division or similar, but more context would be ideal).\n",
      "\n",
      "Without additional information about what \"Pengfei\" refers to directly (e.g., it's someone known personally), the most accurate response based on given details is:\n",
      "\n",
      "\"Pengfei appears to be an individual with diverse interests and profession.\"\n",
      "\n",
      "If you meant for this question in a broader sense, like understanding who Pengfei might represent or referring to specific contexts where \"Pengfei\" would make sense, further clarification could provide more precise insights.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:00:00.856842Z",
     "start_time": "2025-01-20T12:58:51.548336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a query to test the RAG pipeline\n",
    "query2 = (\"What Pengfei loves to do?\")  # Change the query as needed\n",
    "response2 = rag_pipeline(query2)\n",
    "print(\"######## Response from LLM ########\\n\", response2)"
   ],
   "id": "256586eb6f94f0d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Augmented Prompt ########\n",
      "Context: Pengfei playes lots of video games;Pengfei loves to eat appels, Pengfei loves to eat meats;Pengfei want to learn swimming;Pengfei is a data scientist who works for CASD\n",
      "\n",
      "Question: What Pengfei loves to do?\n",
      "Answer:\n",
      "######## Response from LLM ########\n",
      " It seems there are multiple things that Pengfei enjoys doing! \n",
      "\n",
      "Based on the provided context, here's what we can infer:\n",
      "\n",
      "1. **Play video games**: It's mentioned that Pengfei plays lots of video games.\n",
      "2. **Eat apples**: Pengfei loves to eat apples.\n",
      "3. **Eat meats**: Another thing Pengfei loves to do is eat meats.\n",
      "\n",
      "These are the specific activities/enjoyments that were explicitly stated in the context.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab4296ab59a879a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
