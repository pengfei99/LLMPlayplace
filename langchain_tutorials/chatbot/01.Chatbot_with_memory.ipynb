{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Build a chatbot with memory\n",
    "\n",
    "If you build a chatbot with Langchain, you have several ways to incorporate `memory`.\n",
    "- RunnableWithMessageHistory: For more details, you can check this [page](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). There is a tutorial which you can follow [here](https://python.langchain.com/v0.2/docs/tutorials/chatbot/)\n",
    "- BaseChatMessageHistory\n",
    "- LangGraph persistence: https://langchain-ai.github.io/langgraph/concepts/persistence/\n",
    "\n",
    "\n",
    "In this notebook, we will use LangGraph persistence to `persist memory`.\n",
    "\n",
    "**Do not have any folder named as langchain in your project, this will make you receive `module 'langchain' has no attribute 'verbose'`**"
   ],
   "id": "b1ea55cede5e269e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:39:18.591566Z",
     "start_time": "2025-01-21T09:39:18.090902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ],
   "id": "d1ff457ed4d3d7bb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "da6efb63da108772"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Build a basic chatbot\n",
    "\n",
    "In this step, we build a basic model by using `langchain_ollama`. You will notice by default, the LLM model does not have memory of previous conversation. To make it aware of previous conversation, we need to add the previous conversation into the context of the current query.\n"
   ],
   "id": "d6246b42d340e0cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:39:21.115124Z",
     "start_time": "2025-01-21T09:39:21.032532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"steamdj/llama3.1-cpu-only\"\n",
    "model = ChatOllama(model=model_name,temperature=0)"
   ],
   "id": "78c88d554a8ba9b2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:39:21.877397Z",
     "start_time": "2025-01-21T09:39:21.863986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name = \"pengfei\"\n",
    "hm1=f\"Hi! I'm {name}!\"\n",
    "am1=f\"Hello {name}! How can I assist you today?\"\n",
    "hm2=\"What's my name?\""
   ],
   "id": "c2892942aca62e56",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:40:51.342378Z",
     "start_time": "2025-01-21T09:39:22.696364Z"
    }
   },
   "cell_type": "code",
   "source": "model.invoke([HumanMessage(content=f\"{hm1}\")])",
   "id": "66d2e7ef27e85e2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Pengfei! How's your day going so far? Is there something on your mind that you'd like to chat about or is this just a friendly hello?\", additional_kwargs={}, response_metadata={'model': 'steamdj/llama3.1-cpu-only', 'created_at': '2025-01-21T09:40:50.989692469Z', 'done': True, 'done_reason': 'stop', 'total_duration': 88136500980, 'load_duration': 34434082812, 'prompt_eval_count': 18, 'prompt_eval_duration': 5332000000, 'eval_count': 39, 'eval_duration': 48328000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-ad10f293-e961-42d5-ac2c-d899de42e336-0', usage_metadata={'input_tokens': 18, 'output_tokens': 39, 'total_tokens': 57})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:41:18.178379Z",
     "start_time": "2025-01-21T09:40:53.537489Z"
    }
   },
   "cell_type": "code",
   "source": "model.invoke([HumanMessage(content=f\"{hm2}\")])",
   "id": "d64daf84b926650e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm happy to chat with you, but I don't actually know your name. This is the beginning of our conversation, and we haven't had a chance to discuss it yet! If you'd like to share your name with me, I'd be delighted to learn it.\", additional_kwargs={}, response_metadata={'model': 'steamdj/llama3.1-cpu-only', 'created_at': '2025-01-21T09:41:18.168121829Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24299335557, 'load_duration': 346888718, 'prompt_eval_count': 15, 'prompt_eval_duration': 2397000000, 'eval_count': 57, 'eval_duration': 21532000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-3ed26120-12e3-45bd-864f-5375dd7d3681-0', usage_metadata={'input_tokens': 15, 'output_tokens': 57, 'total_tokens': 72})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can notice that, the model has no memory about our conversation. To add memory to the LLM model, we need to build the conversation history manually.",
   "id": "81be83f17af3213e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:02:30.801390Z",
     "start_time": "2025-01-21T09:01:44.114117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.invoke([\n",
    "              HumanMessage(content=f\"{hm1}\"),\n",
    "              AIMessage(content=f\"{am1}\"),\n",
    "              HumanMessage(content=f\"{hm2}\")\n",
    "              ],\n",
    "              )"
   ],
   "id": "f082af592bedd3f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Pengfei!', additional_kwargs={}, response_metadata={'model': 'steamdj/llama3.1-cpu-only', 'created_at': '2025-01-21T09:02:30.620387429Z', 'done': True, 'done_reason': 'stop', 'total_duration': 46436931813, 'load_duration': 17402189426, 'prompt_eval_count': 45, 'prompt_eval_duration': 24298000000, 'eval_count': 8, 'eval_duration': 3649000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-3c110ec0-1721-42b7-8318-4bd55d6dc44b-0', usage_metadata={'input_tokens': 45, 'output_tokens': 8, 'total_tokens': 53})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now we can see that the model give us a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ],
   "id": "fe968f35ee3cbd26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Message persistence\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. It implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "You can visit their [pypi page](https://pypi.org/project/langgraph/) or [github page](https://github.com/langchain-ai/langgraph) for more information.\n",
    "\n",
    "In this tutorial, we will embed our chat model into a minimal LangGraph application which allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ],
   "id": "e4b22a5430634483"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:42:55.573836Z",
     "start_time": "2025-01-21T09:42:55.157266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "id": "65ac63bdcb2745b5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f9cc4a1e23502aa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:42:59.721382Z",
     "start_time": "2025-01-21T09:42:59.701465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "config1 = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ],
   "id": "67702a71801829b0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:43:24.009965Z",
     "start_time": "2025-01-21T09:43:01.136406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the query message\n",
    "print(f\"query message: {hm1}\")\n",
    "\n",
    "input_messages = [HumanMessage(hm1)]\n",
    "output = app.invoke({\"messages\": input_messages}, config1)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ],
   "id": "2ec5cd0520ae4303",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Nice to meet you, Pengfei! How's your day going so far? Is there something on your mind that you'd like to chat about or is this just a friendly hello?\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:46:45.214731Z",
     "start_time": "2025-01-21T09:45:51.458458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the query message\n",
    "print(f\"query message: {hm2}\")\n",
    "\n",
    "# now let's check if the chatbot remembers my name or not\n",
    "input_messages = [HumanMessage(hm2)]\n",
    "output = app.invoke({\"messages\": input_messages}, config1)\n",
    "output[\"messages\"][-1].pretty_print()\n"
   ],
   "id": "5daf2e4bb58edf74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query message: What's my name?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Your name is Pengfei!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:48:17.460380Z",
     "start_time": "2025-01-21T09:47:43.003712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the query message\n",
    "print(f\"query message: {hm2}\")\n",
    "\n",
    "# define a new config\n",
    "config2 =  {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "# now let's check if the chatbot remembers my name or not\n",
    "input_messages = [HumanMessage(hm2)]\n",
    "output = app.invoke({\"messages\": input_messages}, config2)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "695bb34ed77d76e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query message: What's my name?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "I'm happy to chat with you, but I don't actually know your name. This is the beginning of our conversation, and we haven't had a chance to discuss it yet! If you'd like to share your name with me, I'd be delighted to learn it.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can notice that with a new config, the conversation memory stats from zero. Because the chat thread is different from the config1. Now let's check if the memory is still there with config1.",
   "id": "1109a20def42ee60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:51:26.776201Z",
     "start_time": "2025-01-21T09:50:48.381825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the query message\n",
    "print(f\"query message: {hm2}\")\n",
    "\n",
    "# now let's check if the chatbot remembers my name or not\n",
    "input_messages = [HumanMessage(hm2)]\n",
    "output = app.invoke({\"messages\": input_messages}, config1)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "de95efebb78f45c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query message: What's my name?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Your name is Pengfei! (I remember from earlier!)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can notice the chatbot still remember my name with the right thread id. It proves that the conversation history are persisted in the backend database.",
   "id": "1b9b912258ab69f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## 3. Async support",
   "id": "8b8340ebc740f1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
