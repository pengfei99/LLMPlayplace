{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb773fb0-9ca8-4128-bc8e-a007df67004e",
   "metadata": {},
   "source": [
    "## 1) Decortiquer un code de fine-tuning on a Q&A dataset\n",
    "\n",
    "On considère ici le dataset Stanford Question Answering Dataset (SQuAD) et va entrainer **distilbert-base-uncased** sur ce dataset. Décortiquons un code typique pour faire cela, nous en profiterons pour voir les aspects de deep learning dont nous allons avoir besoin.\n",
    "\n",
    "SQuAD est un dataset pour lesquel chaque élément contient trois éléments: le contexte, la question et la réponse. Et dans la version 1 de SQuAD, systématiquement la réponse est contenue dans le contexte. Dans la version 2, ce n'est pas toujours le cas et ca veut dire que le modele doit etre capable de predire qu'il n'y a pas la bonne reponse.\n",
    "- SQuAD 1.0 dataset https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2749099.pdf\n",
    "- SQuAD 2.0 dataset https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15839661.pdf\n",
    "\n",
    "Le concept pour fine-tuned in model sur ce jeu de données Q&A c'est de prendre un contexte + question et tenter de prédire la réponse.\n",
    "**le modèle est donc entraîné à prédire le couple (start_point, end_point)** a partir de contexte + question.\n",
    "\n",
    "La loss est donc assez simple : c'est **L((start_point_pred, end_point_pred), (start_point, end_point))**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f29907-e043-4f59-b46d-495072a646cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# Comme d'habitude, nous telechargeons le dataset, tokenizer et le model\n",
    "dataset = load_dataset(\"squad\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26245bf6-bfb5-4b9e-8330-adf50ba7e32a",
   "metadata": {},
   "source": [
    "Commençons par regarder comment le dataset est structuré pour mieux comprendre la **loss** (fonction de perte) avec laquelle nous allons entraîner le modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de975a3a-a347-4802-a960-8c9643a5d9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le context est : Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "La question est : To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "La reponse est : {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Le context est : {dataset['train']['context'][0]}\" + '\\n')\n",
    "print(f\"La question est : {dataset['train']['question'][0]}\" + '\\n')\n",
    "print(f\"La reponse est : {dataset['train']['answers'][0]}\" + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64fe5c-6f45-458f-9414-0efbd762a027",
   "metadata": {},
   "source": [
    "Pour pouvoir commencer a entrainer un modele, comme les models travaillent directement sur les tokens, il faut tokeniser l'input (context + question) et la reponse. Prenons l'exemple d'une question en particulier : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16830f93-e161-49cf-a356-acaf453d1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['train']['context'][0]\n",
    "question = dataset['train']['question'][0]\n",
    "reponse = dataset['train']['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb38d351-4756-4b67-84e7-dd6cb6e43948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existe-t-il une solution pour tronquer autour de la reponse, connaissant la position de la reponse ?\n",
    "input = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=384,\n",
    "    truncation=\"only_second\",\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "# 1) max_length=384 : la longueur combinee de question + context est tronquee si depasse cette limite.\n",
    "# 2) truncation=\"only_second\" : si question + context est tronquee, on tronque le context.\n",
    "# 3) padding=\"max_length\": On pads la sequence tokenise a la longueur maximale. C'est important pour\n",
    "# s'assurer que tous les inputs ont la meme longueur (batch processing efficace lors de l'entrainement du modele)\n",
    "# 4) return_offsets_mapping=True : on explique juste apres a quoi ils correspondent et pourquoi c'est interessant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121ec07-3b5b-47fb-9fb4-7c452f44352d",
   "metadata": {},
   "source": [
    "La sortie du tokenizer **input** est un dictionnaire avec trois clefs 'input_ids', 'attention_mask', 'offset_mapping':\n",
    "- **'input_ids'** : corresponds aux indices des tokens de context + question. input['input_ids'] est une liste de longueur 384 et qui finit par des zeros (qui correspondent au token [PAD]).\n",
    "- **'offset_mapping'** : corresponds aux indices des tokens dans input['input_ids'] avec (idx_start, idx_end) dans la string initiale context + question.\n",
    "- **'attention_mask'** : Une liste de 0 ou de 1. C'est pour dire quels sont les tokens utiles pour l'attention. Globalement c'est juste pour indiquer au modele qu'il n'y a pas besoin de processer les tokens de [PAD]...\n",
    "\n",
    "\n",
    "Exemple de Text: \"My name is Wolfgang\"\n",
    "Tokens (fake) : [\"My\", \"name\", \"is\", \"Wolfgang\"]\n",
    "\n",
    "Le offset mapping pour cet exemple serait\n",
    "offset_mapping = [(0, 2), (3, 7), (8, 10), (11, 19), (0, 0), ..., (0, 0)]\n",
    "\n",
    "Parce que\n",
    "\n",
    "- Le token \"My\" commence au caractere 0 et termine au caractere 2.\n",
    "- Le token \"name\" commence au caractere 3 et termine au caractere 7.\n",
    "- Le token \"is\" commence au caractere 9 et termine au caractere 10.\n",
    "- Le token \"Wolfgang\" commence au caractere 11 et termine au caractere 19.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d979f87a-12d5-4495-b68f-be2d72937416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'offset_mapping']\n"
     ]
    }
   ],
   "source": [
    "print(list(input.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f64d9e-5e98-4ca3-b42b-6b4eb5783ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(input['input_ids']))\n",
    "print(len(input['attention_mask']))\n",
    "print(input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6bef1-0d76-44e9-9d2f-64d8e2197cce",
   "metadata": {},
   "source": [
    "Pour l'instant nous avons juste tokenisé l'input, il nous reste à tokeniser la réponse pour pouvoir calculer l'erreur du modèle. Idealement, notre objectif est de rajouter deux champs a l'input l'information (start_idx_answer_in_context, end_idx_answer_in_context.\n",
    "\n",
    "Faisons cela sur un exemple en particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc4fd4-0ef5-406a-891a-47411c038449",
   "metadata": {},
   "source": [
    "La méthode **sequence_ids()** de l'objet input renvoit une liste indiquant si chaque token decrit dans input est part \n",
    "- de la question (valeur 0)\n",
    "- du context (valeur 1)\n",
    "- est un special tokens (valeur None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf6d5aa-17ca-4a15-ab0b-18bb23d93f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# Le premier None correspong a BOS tandis que le second corresponds a la transition de la question vers le contexte.\n",
    "# Les derniers corresponds au [PAD] tokens.\n",
    "sequence_ids = input.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6035f8-f944-4abf-940a-3cc30116c70a",
   "metadata": {},
   "source": [
    "A partir de cette liste on peut trouver l'index du premier token associé au contexte et trouver l'indice du dernier token associé au contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2711616-1d73-4209-948e-60ad56c35ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context starts at token 17 and ends at token 174\n"
     ]
    }
   ],
   "source": [
    "# Find the start index of the context\n",
    "context_start = sequence_ids.index(1)\n",
    "\n",
    "# Find the end index of the context\n",
    "context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "print(f'The context starts at token {context_start} and ends at token {context_end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b16f3-ee44-448b-8ccd-187d80f1cce7",
   "metadata": {},
   "source": [
    "Il est intéressant de reconstruire à partir du champs input_ids de input la liste des tokens. Ca permet de comprendre aussi les differents special tokens qui sont utilises. On voit\n",
    "- **[CLS]** : nous n'en parlons pas pour l'instant.\n",
    "- **[SEP]** : c'est le token de separation entre la question et le contexte.\n",
    "- **[PAD]** : c'est le token de padding.\n",
    "\n",
    "Noter que dans le processus de tokenization process utilisé par BERT ou DistilBERT le symbole **##** est utilisé pour indiquer le cas lorsqu'un token est sous-mot ou la continuation d'un mot précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96107c8b-2023-4445-b4f8-e0341f3d7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', '1858', 'in', 'lou', '##rdes', 'france', '?', '[SEP]', 'architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 've', '##ni', '##te', 'ad', 'me', 'om', '##nes', '\"', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'gr', '##otto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'lou', '##rdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputed', '##ly', 'appeared', 'to', 'saint', 'bern', '##ade', '##tte', 'so', '##ub', '##iro', '##us', 'in', '1858', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# On peut a partir de input_ids reconstruire la context + question ...\n",
    "\n",
    "# On commence par passer des indices des tokens a une liste de token\n",
    "tokens = tokenizer.convert_ids_to_tokens(input['input_ids'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0c626b-6d24-453b-8e16-b69f20a65973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed prompt without special tokens and padding: to whom did the virgin mary allegedly appear in 1858 in lourdes france ? architecturally , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the grotto , a marian place of prayer and reflection . it is a replica of the grotto at lourdes , france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary .\n"
     ]
    }
   ],
   "source": [
    "# Filter out special tokens and padding\n",
    "filtered_tokens = [token for token in tokens if token not in tokenizer.all_special_tokens]\n",
    "\n",
    "# Manually join the tokens into a string\n",
    "prompt = ' '.join(filtered_tokens).replace(' ##', '')\n",
    "\n",
    "print(\"Reconstructed prompt without special tokens and padding:\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f9d5c-c92a-4e23-b256-0287d663f514",
   "metadata": {},
   "source": [
    "Maintenant que nous avons compris un petit mieux les différents champs de input et la structure de context/question/answer, revenons à notre objectif principal : **processer l'input pour donner le couple (start, end) de la réponse dans le contexte et dans le cas ou ce n'est pas possible comprendre ce qu'il faut donner en output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55642c01-9b3d-4ba7-88fb-0e9052415e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['train']['context'][0]\n",
    "question = dataset['train']['question'][0]\n",
    "answer = dataset['train']['answers'][0]\n",
    "\n",
    "input = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dfa6b16-2d64-4853-a509-9917b8a9df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut deja commencer par trouver la position (en terme de nombre de caracteres \n",
    "# de la reponse dans le context.\n",
    "start_pos = context.find(answer['text'][0])\n",
    "end_pos = start_pos + len(answer['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd5a6ba-acf8-48e6-bb4f-0aa787e0880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[start_pos:end_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d7c3ab3-77f9-4789-9ebc-1ca81e353eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En fait, le dictionnaire answer contient aussi le champs answer.\n",
    "start_char = answer[\"answer_start\"][0]\n",
    "end_char = start_char + len(answer[\"text\"][0])\n",
    "# Attention: on raisonne sur les string et pas sur les tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e180f36-fccc-4801-9bb1-8677aaa2c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c8e7a-75f6-411c-a6ee-a9b4c3e9371d",
   "metadata": {},
   "source": [
    "Il nous faut maintenant gerer la situation ou la partie du contexte qui contient l'information a ete tronque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a7a3af9-2b1b-4d6e-af0b-88f5e4acb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping = input[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc0bb548-384d-4f86-b293-5b2320061028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce0d8036-ecbd-448e-a30e-11e1a5474709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "176d8395-764a-47dd-afc5-e5e3d5d55fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_ids = inputs.sequence_ids(i)\n",
    "# context_start = sequence_ids.index(1)\n",
    "# context_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1f8e1-dd28-4ae5-985d-f0c6f07c9411",
   "metadata": {},
   "source": [
    "Plusieurs situations peuvent subvenir maintenat :\n",
    "- **Si le contexte a été tronqué par le tronque par le tokenizer, la partie du contexte contenant la réponse n'existe plus et nous sommes dans le cas ou la réponse à la question n'est pas contenue dans le contexte ! Dans ce cas, nous disons que (start, end) = (0,0), ce qui revient à assigner la réponse au token CLS, on comprend enfin l'utilité du token CLS !**\n",
    "- La condition `offset[context_start][0] > end_char or offset[context_end][1] < start_char` vérifie exactement si nous sommes dans la situation décrite précédemment.\n",
    "- Ensuite il faut réfléchir un petit peu pour déduire à partir des positions des caractères (start, end) de la réponse, la position des indices de token dans le prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d7929d5-60e0-429c-b53e-97bdea06bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if offset_mapping[context_start][0] > end_char or offset_mapping[context_end][1] < start_char:\n",
    "    start_position = 0\n",
    "    end_position = 0\n",
    "else:\n",
    "    start_position = next(i for i, x in enumerate(offset_mapping) if x[0] <= start_char < x[1])\n",
    "    end_position = next(i for i, x in enumerate(offset_mapping) if x[0] < end_char <= x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b5b76-24eb-4ef3-8d6c-1f1c3bd252ab",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons écrire la fonction qui va faire ce processing au niveau du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47a284f7-8e5f-4a86-a62e-98b8eebe8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tqdm\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_function(examples: datasets.DatasetDict):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    contexts = [c.strip() for c in examples['context']]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "    \n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_pos = [i for i, x in enumerate(offset) if x[0] <= start_char <= x[1]][0]\n",
    "            end_pos = [i for i, x in enumerate(offset) if x[0] <= end_char <= x[1]][0]\n",
    "            start_positions.append(start_pos)\n",
    "            end_positions.append(end_pos)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ff6dd22-d100-494a-b582-c904d9df0688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9b319a6d3747a3b3329aed128ce652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the preprocessing function to the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f641e7e-6103-4681-9d97-86c9b6cc781e",
   "metadata": {},
   "source": [
    "Le processus d'entrainement lui est tres peu detaille :\n",
    "Le trainer de huggingFace : https://huggingface.co/docs/transformers/en/main_classes/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "096a9990-4b1b-4200-b3b9-3fc6d104f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX tensorboard > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6569a7fa-bf08-46a8-b7d5-22d95279f633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11867), started 0:00:30 ago. (Use '!kill 11867' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aec26592-b021-4020-a524-3017c985f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329a194b-8844-457b-bee9-2bedba9440d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='16425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   29/16425 00:25 < 4:15:17, 1.07 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# Le model HF Transformers a entrainer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# les arguments d'entrainement\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],   \u001b[38;5;66;03m# le dataset d'entrainement\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],   \u001b[38;5;66;03m# le dataset d'evaluation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/transformers/trainer.py:2273\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from tensorboardX import SummaryWriter  # or use `torch.utils.tensorboard.SummaryWriter` if using PyTorch\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # folder pour stoker les checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # evaluer apres chaque epoque\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=16,  # batch size pour entrainement\n",
    "    per_device_eval_batch_size=16,   # batch size pour evaluation\n",
    "    num_train_epochs=3,              # nombre d'epoques\n",
    "    weight_decay=0.01,               # regularization L2 ...\n",
    "    logging_dir='./logs',            # pour stocker les logs auxquels nous allons acceder via tensorboard\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # Le model HF Transformers a entrainer\n",
    "    args=training_args,                  # les arguments d'entrainement\n",
    "    train_dataset=encoded_dataset['train'],   # le dataset d'entrainement\n",
    "    eval_dataset=encoded_dataset['validation'],   # le dataset d'evaluation\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef1713-8986-419b-8065-2649b0e80871",
   "metadata": {},
   "source": [
    "## 2) Parameter Efficient Fine-tuning on a Q&A dataset\n",
    "\n",
    "Quelques idees derriere QLoRA ou LoRA:\n",
    "- On **freeze** un grand nombre de poids du modeles. Cela evite notamment le *catastrophic forgetting*.\n",
    "- LoRA remplace certaines matrices de poids par des matrices de low-rank dans les layers des transformers. La matrice initial de poids n'est pas modifiee mais plutot les deux matrices qui composent chacun des poids.\n",
    "- LoRA ne fait cela que pour certains types de matrices de poids (typiquement les matrices associes au calcul de score d'attention).\n",
    "- En plus de tout cela, QLora vient quantizer les poids a INT4.\n",
    "\n",
    "\n",
    "Quelques tres bonnes ressources pour comprendre tous les details: \n",
    "- https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766\n",
    "- https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95be41c2-1f3c-42d3-ae21-43a21a9c488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate peft  > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8362fea9-f8b9-48a3-b466-1cd4f45ee7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, TrainingArguments, Trainer\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf28c10-bb85-4f51-86a8-e130357528e8",
   "metadata": {},
   "source": [
    "Les differents packages pour facilement faire du fine-tuning de LLM\n",
    "- **datasets** = package de HuggingFace pour avoir acces a de nombreux datasets.\n",
    "- **peft** = Parameter Efficient Fine-Tuning. Developpe par HuggingFace.\n",
    "- **AutoModelForCausalLM** = C'est une facon de retrouver l'architecture du pretrained model a partir du nom du modele. Developpe par HuggingFace.\n",
    "- **transformers** = un des packages les plus connu de HuggingFace qui regroupe de nombreuses fonctionnalites autour des transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04e7477f-b964-445d-8d39-d9460ae75bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ca25d-5c60-40e8-9dfd-1307ff7b84bb",
   "metadata": {},
   "source": [
    "We have to prepare the models for the pretraining in the following ways :\n",
    "- **model.train()** : preparer le modele pour l'entrainement, et c'est surtout a faire lorsque le modele a des layers de dropout.\n",
    "- **model.gradient_checkpointing_enable()** : permet de faire du **gradient checkpointing**. Il s'agit d'une technique dans les reseaux de neurones pour reduire l'usage de memoire pendant l'entrainement.\n",
    "- **prepare_model_for_kbit_training** : préparer des modèles pour un entraînement avec une précision réduite, en particulier avec des largeurs de bits inférieures (par exemple, un entraînement 8 bits ou 4 bits).\n",
    "\n",
    "**Dropout** = drop out pendant l'entrainement un certain nombre de neurones des reseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdcb4393-0e26-4919-9c53-e3176d3638df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train() # model in training mode (dropout modules are activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94631ad5-3596-43e7-89a1-e622b5b7347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af008337-4f4e-43ba-9904-86283e567faf",
   "metadata": {},
   "source": [
    "**Il nous faut maintenant configurer le type d'entrainement LoRA que nous avons envie de pratiquer.**\n",
    "\n",
    "We have to find the **target modules** that are the best to use in order to perform the LoRA technique. En effet, il faut preciser la configuration de **LoraConfig** et notamment la variable **target_modules**.\n",
    "\n",
    "- **lora_alpha** = c'est un **scaling factor** pour injecter les matrices low-rank dans l'entrainement.\n",
    "- **target_modules** = Les parties du reseau de neurone ou on va modifier les matrices de poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "370e21bb-3e07-4b59-b5eb-920724e067ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CE CODE DONNE UNE ERREUR PARCE QUE J\"AI MIS N'IMPORTE QUOI POUR LA VARIABLE TARGET_MODULES\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the adaptation matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"],  # Modules to apply LoRA to\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# On applique LoRA au model = on ajoute nos matrices low-rank que l'on va apprendre\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db5e8e-9324-4ac0-bc9b-b032cbfc34e4",
   "metadata": {},
   "source": [
    "Pour avoir le nom de toutes les layers voici comment on procede "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ff2f71d-2d68-4b87-8275-133c5ff08b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "base_model\n",
      "base_model.model\n",
      "base_model.model.distilbert\n",
      "base_model.model.distilbert.embeddings\n",
      "base_model.model.distilbert.embeddings.word_embeddings\n",
      "base_model.model.distilbert.embeddings.position_embeddings\n",
      "base_model.model.distilbert.embeddings.LayerNorm\n",
      "base_model.model.distilbert.embeddings.dropout\n",
      "base_model.model.distilbert.transformer\n",
      "base_model.model.distilbert.transformer.layer\n",
      "base_model.model.distilbert.transformer.layer.0\n",
      "base_model.model.distilbert.transformer.layer.0.attention\n",
      "base_model.model.distilbert.transformer.layer.0.attention.dropout\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.0.attention.out_lin\n",
      "base_model.model.distilbert.transformer.layer.0.sa_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.0.ffn\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.dropout\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.activation\n",
      "base_model.model.distilbert.transformer.layer.0.output_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.1\n",
      "base_model.model.distilbert.transformer.layer.1.attention\n",
      "base_model.model.distilbert.transformer.layer.1.attention.dropout\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.1.attention.out_lin\n",
      "base_model.model.distilbert.transformer.layer.1.sa_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.1.ffn\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.dropout\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.activation\n",
      "base_model.model.distilbert.transformer.layer.1.output_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.2\n",
      "base_model.model.distilbert.transformer.layer.2.attention\n",
      "base_model.model.distilbert.transformer.layer.2.attention.dropout\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.2.attention.out_lin\n",
      "base_model.model.distilbert.transformer.layer.2.sa_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.2.ffn\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.dropout\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.activation\n",
      "base_model.model.distilbert.transformer.layer.2.output_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.3\n",
      "base_model.model.distilbert.transformer.layer.3.attention\n",
      "base_model.model.distilbert.transformer.layer.3.attention.dropout\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.3.attention.out_lin\n",
      "base_model.model.distilbert.transformer.layer.3.sa_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.3.ffn\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.dropout\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.activation\n",
      "base_model.model.distilbert.transformer.layer.3.output_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.4\n",
      "base_model.model.distilbert.transformer.layer.4.attention\n",
      "base_model.model.distilbert.transformer.layer.4.attention.dropout\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin\n",
      "base_model.model.distilbert.transformer.layer.4.sa_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.4.ffn\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.dropout\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.activation\n",
      "base_model.model.distilbert.transformer.layer.4.output_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.5\n",
      "base_model.model.distilbert.transformer.layer.5.attention\n",
      "base_model.model.distilbert.transformer.layer.5.attention.dropout\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.base_layer\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_dropout\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_dropout.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_embedding_A\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_embedding_B\n",
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin\n",
      "base_model.model.distilbert.transformer.layer.5.sa_layer_norm\n",
      "base_model.model.distilbert.transformer.layer.5.ffn\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.dropout\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.activation\n",
      "base_model.model.distilbert.transformer.layer.5.output_layer_norm\n",
      "base_model.model.qa_outputs\n",
      "base_model.model.dropout\n"
     ]
    }
   ],
   "source": [
    "# Print all submodule names\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04898952-581a-437d-858e-4e79bb7a055c",
   "metadata": {},
   "source": [
    "Les matrices QKV sont les éléments auxquels nous avons envie d'appliquer les modification LoRA parce que ce sont des matrices a peu pres quadratiques en la taille de la contexte window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91149d59-565e-40e5-b7d3-37e7344f8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the adaptation matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"],  # Modules to apply LoRA to\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f311e258-69d2-4851-a192-7ccf32468cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 221,184 || all params: 66,585,602 || trainable%: 0.3322\n"
     ]
    }
   ],
   "source": [
    "# trainable parameter count\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073bf3f7-c888-4b97-99d1-0f7cf1991de7",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons entrainer comme precedement le model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d07e480-0ba2-49fb-b31b-4d983e18b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 29\u001b[0m\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                                 \u001b[38;5;66;03m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                          \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],      \u001b[38;5;66;03m# training dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# evaluation dataset\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/transformers/trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2230\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/transformers/data/data_collator.py:158\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    156\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from tensorboardX import SummaryWriter  # or use `torch.utils.tensorboard.SummaryWriter` if using PyTorch\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                                 # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                          # training arguments, defined above\n",
    "    train_dataset=encoded_dataset['train'],      # training dataset\n",
    "    eval_dataset=encoded_dataset['validation'],  # evaluation dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63e10c31-b7b9-4bbf-bc6f-5de95a642aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "encoded_dataset['train'][0]['start_positions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faacfb-107c-49b7-a466-a2d42209f28e",
   "metadata": {},
   "source": [
    "## 4) Travailler sur des taches autres que du Q&A.\n",
    "\n",
    "Nous pouvons aussi fine-tuner des modeles sur d'autres datasets \n",
    "- NLI (Natural Language Inference)\n",
    "- STS (Semantic Textual Similarity)\n",
    "\n",
    "**Exercice**: Faire du fine-tuning sur le dataset **snli**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add23ed-2c2d-43e8-8bcf-74c80f18bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f281d-808b-46c9-9616-ae7518e1814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560dd2d-3fc9-4966-be4d-7e8bf37a680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels pour NLI: entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9969039-9ed2-4e3c-8244-fb0c7dbf9d35",
   "metadata": {},
   "source": [
    "Ici la preprocess_function est beaucoup plus simple (NLI n'est qu'une tache de classification a trois classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3da70-c5a1-4b31-a5fb-07fa769ee4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length')\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180ba66-8ccc-4fef-a591-6eccaf47e275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828653e2-2d98-4a7f-9a67-45c840522eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd3a81-920f-478c-af00-c814ba31e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121a78c-dd92-48f3-8174-bd41954e8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfc8b1-0b58-43c5-82ce-f756d385c1c4",
   "metadata": {},
   "source": [
    "## 4) Fine-tuning sur d'autres Q&A dataset\n",
    "\n",
    "Il y a  de nombreux autres jeux de donnees de Q&A ou le type de taches est semblable a celui pour SQuAD: predire le couple (start_pos, end_pos) . Par exemple :\n",
    "- https://huggingface.co/datasets/google-research-datasets/natural_questions?row=0 NQ : Natural Questions.\n",
    "- https://huggingface.co/datasets/mandarjoshi/trivia_qa?row=8 TriviaQ\n",
    "\n",
    "Mais une difficulte est que chacun de ces datasets a des structures differentes. **Exercice**: pour deux autres datasets **trivia-qa-triplet** et **Quora**, les explorer et comprendre quelle pourrait la tache d'entrainement.\n",
    "- https://huggingface.co/datasets/toughdata/quora-question-answer-dataset Quora.\n",
    "https://huggingface.co/datasets/sentence-transformers/trivia-qa-triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be2e5f-57bf-4261-81d4-a4e7393363be",
   "metadata": {},
   "source": [
    "Commencons par **trivia-qa-triplet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e212272-8c44-420b-86e7-d47ef92d96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"trivia_qa\", \"rc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb99530-bac9-4425-973d-e6be0950e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trivia_qa\", \"rc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555667c6-89d7-40fa-9fbe-26972a33302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8c872-2af4-4c68-bc05-dce91d23664a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e3d093-e272-48cf-83b2-6f3360d93022",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fcd99-8313-4a91-b367-0b43a742b148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
