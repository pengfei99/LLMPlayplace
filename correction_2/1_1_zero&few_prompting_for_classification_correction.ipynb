{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "l7AXcB5mzu5n",
      "metadata": {
        "id": "l7AXcB5mzu5n"
      },
      "source": [
        "# Zero-shot learning et few-shot learning avec un LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52cd977-7467-44f7-b469-6199d4d340d5",
      "metadata": {
        "id": "d52cd977-7467-44f7-b469-6199d4d340d5"
      },
      "source": [
        "Dans ce TP, nous allons expérimenter les concepts de zero-shot et few-shot learning qui deviennent de plus en plus importants pour le changement de paradigme en Intelligence Artificielle basé sur l'apprentissage auto-supervisé.\n",
        "\n",
        "Que signifient zero-shot et few-shot learning ?\n",
        "\n",
        "- En **zero-shot learning**, on utilise un modèle pré-entraîné (= un modèle de fondation) pour une classe d'objets ou d'inputs qu'il n'a jamais vu. Exemple : on demande à un modèle entraîné à détecter les voitures et les humains dans une image de détecter tous les bâtiments sans jamais lui avoir donné de données annotés de bâtiments en entraînement.\n",
        "\n",
        "- En **few-shot learning**, on donne par contre quelques exemples (seulement) à ce modèle pré-entraîné.\n",
        "\n",
        "Il y a de nombreuses façons de faire du zero-shot/few-shot learning. Tout dépend du type de données que l'on considère (texte, image, vidéo ou autre) et de la structure du modèle de fondation que l'on considère.\n",
        "\n",
        "En pratique, lorsqu'il possible de remplir une tâche en mode zero-shot learning, c'est la première chose que l'on teste. Selon la qualité du résultat, on teste ensuite le few-shot learning, avant d'éventuellement se lancer dans du fine-tuning ou des méthodes plus complexes d'adaptation de modèle.\n",
        "\n",
        "Quelques exemples de stratégies de zero/few-shot learning pour les LLMs :\n",
        "- **zero/few-shot prompting** : on ajoute des exemples dans le contexte du prompt ;\n",
        "\n",
        "- Faire du fine-tuning avec quelques exemples ;\n",
        "\n",
        "- Le Reinforcement Learning from Human Feedback (RLHF) est une méthode de few-shot learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db42dc29-4156-426d-8587-0c9f846ca557",
      "metadata": {
        "id": "db42dc29-4156-426d-8587-0c9f846ca557",
        "outputId": "a869f9a0-761a-4889-a5cf-b1cbff796bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YIv-mk9yz1oB",
      "metadata": {
        "id": "YIv-mk9yz1oB"
      },
      "source": [
        "## 1) Exemple : zero-shot prompting pour classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51e165c-49ee-4307-8a87-9f8b2eee6785",
      "metadata": {
        "id": "e51e165c-49ee-4307-8a87-9f8b2eee6785"
      },
      "source": [
        "Ici nous allons donner un exemple d'utilisation de zero-shot prompting pour classifier des critiques (*reviews*) de films.\n",
        "\n",
        "Le zero-shot ou few-shot prompting est souvent aussi appelé \"in-context learning\", mais dans un contexte assez spéficique aux LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d99139f-b52a-4dfc-9327-ba534b342f9c",
      "metadata": {
        "id": "0d99139f-b52a-4dfc-9327-ba534b342f9c"
      },
      "source": [
        "On commence par écrire la fonction qui va nous permettre de classifier via un prompt si une review est positive ou négative. Nous nous basons largement sur les éléments de code vus dans le Jupyter notebook précédent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91fa961-c604-4f3f-8994-ca7211dc9deb",
      "metadata": {
        "id": "f91fa961-c604-4f3f-8994-ca7211dc9deb"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from credentials.keys import OPENAI_API_KEY\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def send_prompt_with_context(\n",
        "        messages: list[Dict],\n",
        "        model: str = 'gpt-4o-mini',\n",
        "        temperature: float = 0.1) -> str:\n",
        "    '''\n",
        "    Envoi d'un message au modèle.\n",
        "    '''\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.1,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    return resp.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190ac5fe",
      "metadata": {
        "id": "190ac5fe"
      },
      "source": [
        "Essayons sur un exemple simple :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd4c2c9-a947-45d0-85c7-4b680e0d6d80",
      "metadata": {
        "id": "6cd4c2c9-a947-45d0-85c7-4b680e0d6d80",
        "outputId": "35a5ad22-40bb-4144-d806-8aadf7cb95b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n  \"gpt_sentiment\": \"neg\"\\n}'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"Is the movie associated with this review positive or negative ?\"\n",
        "review = \"Very bad movie\"\n",
        "\n",
        "system_msg = f\"\"\"\n",
        "You are an assistant that classifies reviews according to their sentiment. \\\n",
        "Respond in json format with the key: gpt_sentiment, for example: '{{\\n  \"gpt_sentiment\": \"pos\"\\n}}'.\\\n",
        "The value for gpt_sentiment should only be either pos or neg without punctuation: pos if the review is positive, neg otherwise.\\\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_msg},\n",
        "    {\"role\": \"system\", \"content\": f\"Consider the following review {review}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "]\n",
        "\n",
        "other_answer = send_prompt_with_context(messages)\n",
        "other_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd86b113-55a5-4fb8-8f1e-b146b24d9cb1",
      "metadata": {
        "id": "cd86b113-55a5-4fb8-8f1e-b146b24d9cb1"
      },
      "source": [
        "La réponse étant donnée en format JSON, il faut l'interpréter via la librarie `json` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96900308-82b6-4e14-8927-4079ec8cd605",
      "metadata": {
        "id": "96900308-82b6-4e14-8927-4079ec8cd605"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "data = json.loads(other_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b401b8-8f47-4049-8b42-010136ee1a40",
      "metadata": {
        "id": "84b401b8-8f47-4049-8b42-010136ee1a40"
      },
      "source": [
        "Téléchargeons maintenant le dataset [**Large Movie Review**](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "Il est structuré de la façon suivante :\n",
        "\n",
        "```\n",
        "aclImdb/\\\n",
        "├── test/ \\\n",
        "│   ├── neg/\\\n",
        "│   └── pos/\\\n",
        "├── train/\\\n",
        "│   ├── neg/\\\n",
        "│   ├── pos/\\\n",
        "│   └── unsup/\\\n",
        "└── imdb.vocab\n",
        "```\n",
        "\n",
        "Le folder `unsup/` dans `train/` correspond seulement au cas où nous voudrions faire de l'entraînement non-supervisé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ecdeec5-b37f-4e6b-9cd4-3b3b8aa1a129",
      "metadata": {
        "id": "1ecdeec5-b37f-4e6b-9cd4-3b3b8aa1a129",
        "outputId": "c663d1a4-5439-4c65-9be2-44fff85de356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-13 11:33:13--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  11.8MB/s    in 11s     \n",
            "\n",
            "2024-10-13 11:33:25 (7.23 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('aclImdb'):\n",
        "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4997f20-7cd0-4d4e-9185-f26f6bdc7af1",
      "metadata": {
        "id": "f4997f20-7cd0-4d4e-9185-f26f6bdc7af1"
      },
      "source": [
        "On charge le dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4fa1f8-b1ea-4fe3-addd-e033316ce87f",
      "metadata": {
        "id": "5d4fa1f8-b1ea-4fe3-addd-e033316ce87f",
        "outputId": "cd47f9da-c46b-4b5f-f7b7-8f2cc68764e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 40 reviews.\n",
            "                                              review sentiment\n",
            "0  Yes, it's another great magical Muppet's movie...       pos\n",
            "1  This movie is outrageous, funny, ribald, sophi...       pos\n",
            "2  ...would probably be the best word to describe...       pos\n",
            "3  Dog Bite Dog isn't going to be for everyone, b...       pos\n",
            "4  WWE Armageddon, December 17, 2006 -- Live from...       pos\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_imdb_dataset(dataset_path: str, nbr_max_per_class: int = 25):\n",
        "    '''\n",
        "    Inputs:\n",
        "    -------\n",
        "        nbr_max_per_class (int) : nombre maximum de reviews à charger pour\n",
        "            chaque classe (positive et négative). On ne veut pas pour cet\n",
        "            exercice considérer toutes les reviews pour éviter de payer\n",
        "            trop cher en OpenAI API :-)\n",
        "    '''\n",
        "    data = {'review': [], 'sentiment': []}\n",
        "\n",
        "    for sentiment in ['pos', 'neg']:\n",
        "        path = os.path.join(dataset_path, sentiment)\n",
        "        for i, file_name in enumerate(os.listdir(path)):\n",
        "            with open(os.path.join(path, file_name), 'r', encoding='utf-8') as file:\n",
        "                data['review'].append(file.read())\n",
        "                data['sentiment'].append(sentiment)\n",
        "            if i >= nbr_max_per_class - 1:\n",
        "                break\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Load training and testing datasets\n",
        "train_dataset = load_imdb_dataset('aclImdb/train')\n",
        "test_dataset = load_imdb_dataset('aclImdb/test')\n",
        "\n",
        "print(f'Loaded {len(train_dataset)} reviews.')\n",
        "print(train_dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4f1711-9b38-4582-ae7c-eafc22da2312",
      "metadata": {
        "id": "ca4f1711-9b38-4582-ae7c-eafc22da2312"
      },
      "source": [
        "Testons le prompt sur un exemple de review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae521fea-7212-4bd3-a721-3c3f5546ffdb",
      "metadata": {
        "id": "ae521fea-7212-4bd3-a721-3c3f5546ffdb",
        "outputId": "aa6f7704-d839-49e4-e58e-6ca684f1fa7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the review: \n",
            " This movie is outrageous, funny, ribald, sophisticated & hits the bullseye where 99 % of Hollywood movies don't even make the target. Paul Bartel should be recognized as one of the great directors of this or any era. He's the American Renoir & Bunuel _ combined!!! Glad I have the videodisc.\n",
            "It is labeled as pos in the dataset.\n"
          ]
        }
      ],
      "source": [
        "review = train_dataset['review'][1]\n",
        "label = train_dataset['sentiment'][1]\n",
        "\n",
        "print(f'This is the review: \\n {review}')\n",
        "print(f'It is labeled as {label} in the dataset.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fde7868-3485-4fa1-83e3-6a4af8300ac3",
      "metadata": {
        "id": "7fde7868-3485-4fa1-83e3-6a4af8300ac3"
      },
      "source": [
        "Testons maintenant un prompt pour faire de la zero-shot classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7309e787-ac87-47ee-9e30-59fab8fe0ddd",
      "metadata": {
        "id": "7309e787-ac87-47ee-9e30-59fab8fe0ddd",
        "outputId": "e6722340-f4f6-4d91-fe9e-7e6cc3249520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'gpt_sentiment': 'pos'}\n",
            "This review has been classified as pos by GPT and was pos in the dataset\n"
          ]
        }
      ],
      "source": [
        "question = \"Is the movie associated with this review positive or negative ?\"\n",
        "\n",
        "system_msg = f\"\"\"\n",
        "You are an assistant that classifies reviews according to their sentiment. \\\n",
        "Respond in json format with the key: gpt_sentiment, for example: '{{\\n  \"gpt_sentiment\": \"pos\"\\n}}'.\\\n",
        "The value for gpt_sentiment should only be either pos or neg without punctuation: pos if the review is positive, neg otherwise.\\\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_msg},\n",
        "    {\"role\": \"system\", \"content\": f\"Consider the following review {review}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "]\n",
        "\n",
        "other_answer = send_prompt_with_context(messages)\n",
        "\n",
        "# On vérifie le format du prompt.\n",
        "wrong_format = False\n",
        "try:\n",
        "    data = json.loads(other_answer)\n",
        "except:\n",
        "    print('Could not load the data to json.')\n",
        "\n",
        "if 'gpt_sentiment' not in data.keys():\n",
        "    wrong_format = True\n",
        "elif data['gpt_sentiment'] not in ['pos', 'neg']:\n",
        "    wrong_format = True\n",
        "\n",
        "if wrong_format:\n",
        "    print('GPT did not provide the right format for the answer')\n",
        "else:\n",
        "    print(f'This review has been classified as {data['gpt_sentiment']} by GPT and was {label} in the dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63d34d0-5a1c-454e-99e3-b77b772a0881",
      "metadata": {
        "id": "f63d34d0-5a1c-454e-99e3-b77b772a0881"
      },
      "source": [
        "**Exercice** : Calculer la performance de cette approche. Calculer le nombre de cas où le format de la réponse n'est pas correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d4dfea-33aa-4e21-9be5-d584a546be79",
      "metadata": {
        "id": "71d4dfea-33aa-4e21-9be5-d584a546be79"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "\n",
        "def make_prediction(review: str) -> Union[str, bool]:\n",
        "    '''\n",
        "    On package dans une fonction les étapes précédentes.\n",
        "    '''\n",
        "    question = \"Is the movie associated with this review positive or negative ?\"\n",
        "\n",
        "    system_msg = f\"\"\"\n",
        "    You are an assistant that classifies reviews according to their sentiment. \\\n",
        "    Respond in json format with the key: gpt_sentiment, for example: '{{\\n  \"gpt_sentiment\": \"pos\"\\n}}'.\\\n",
        "    The value for gpt_sentiment should only be either pos or neg without punctuation: pos if the review is positive, neg otherwise.\\\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_msg},\n",
        "        {\"role\": \"system\", \"content\": f\"Consider the following review {review}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    other_answer = send_prompt_with_context(messages)\n",
        "\n",
        "    # On vérifie le format du prompt.\n",
        "    wrong_format = False\n",
        "    try:\n",
        "        data = json.loads(other_answer)\n",
        "    except:\n",
        "        data = other_answer\n",
        "        wrong_format = True\n",
        "        return False\n",
        "\n",
        "    if 'gpt_sentiment' not in data.keys():\n",
        "        wrong_format = True\n",
        "    elif data['gpt_sentiment'] not in ['pos', 'neg']:\n",
        "        wrong_format = True\n",
        "\n",
        "    if wrong_format:\n",
        "        return False\n",
        "    else:\n",
        "        return data['gpt_sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a93ef8-4e01-468c-bf28-ecc19ad818b4",
      "metadata": {
        "id": "14a93ef8-4e01-468c-bf28-ecc19ad818b4"
      },
      "source": [
        "On calcule l'accuracy sur tout le dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9c9848-d3ac-4552-88de-69ba33e47e37",
      "metadata": {
        "id": "5a9c9848-d3ac-4552-88de-69ba33e47e37"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "review = train_dataset['review'][0]\n",
        "label = train_dataset['sentiment'][0]\n",
        "\n",
        "def accuracy_dataset(train_dataset: pd.DataFrame):\n",
        "    '''\n",
        "    train_dataset is a pandas dataframe with two columns ('review' and 'sentiment').\n",
        "    '''\n",
        "    nbr_exemple = train_dataset.shape[0]\n",
        "    nbr_bad_format = 0\n",
        "    correct = 0\n",
        "    for i in tqdm.tqdm(range(nbr_exemple)):\n",
        "        pred = make_prediction(train_dataset['review'][i])\n",
        "        if type(pred) == bool:\n",
        "            nbr_bad_format += 1\n",
        "        else:\n",
        "            correct += (train_dataset['sentiment'][i] == pred)\n",
        "    print(f'{100*nbr_bad_format/nbr_exemple}% of examples are wrongly formatted.')\n",
        "    print(f'This method has {100*correct/nbr_exemple}% accuracy.')\n",
        "    return nbr_bad_format, correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008bd4b3-430a-41b8-beb2-89f795403b2a",
      "metadata": {
        "id": "008bd4b3-430a-41b8-beb2-89f795403b2a",
        "outputId": "da50a879-4ad8-41e9-e57f-695d7bbe0bac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:04<00:00,  1.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0% of examples are wrongly formatted.\n",
            "This method has 80.0% accuracy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "res = accuracy_dataset(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a032e1-7501-4a26-a85a-d200cf27c014",
      "metadata": {
        "id": "93a032e1-7501-4a26-a85a-d200cf27c014",
        "outputId": "582f2f20-43fc-42ff-f63a-4ea85d432065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0% of examples are wrongly formatted.\n",
            "This method has 10.0% accuracy.\n"
          ]
        }
      ],
      "source": [
        "nbr_exemple = train_dataset.shape[0]\n",
        "nbr_bad_format, correct = res\n",
        "print(f'{100*nbr_bad_format/nbr_exemple}% of examples are wrongly formatted.')\n",
        "print(f'This method has {100*correct/nbr_exemple}% accuracy.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cQ3CfgQ0QRS",
      "metadata": {
        "id": "6cQ3CfgQ0QRS"
      },
      "source": [
        "## TP) Exemple de few-shot prompting pour classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a76c3a8-9107-43c8-be58-08e0975057d7",
      "metadata": {
        "id": "3a76c3a8-9107-43c8-be58-08e0975057d7"
      },
      "source": [
        "Nous allons maintenant effectuer la même chose, sauf que nous allons faire du few-shot prompting. Le TP est très peu encadré. L'objectif est que vous adaptiez le code précédent pour faire du few-shot prompting.\n",
        "\n",
        "Quelques étapes suggérées :\n",
        "\n",
        "- Formatter un objet `Message` pour pouvoir inclure différents exemples de review.\n",
        "\n",
        "- Calculer la performance de cette approche.\n",
        "\n",
        "D'autres aspects à expérimenter:\n",
        "\n",
        "- Quel est l'effet d'augmenter la température sur les performances ?\n",
        "\n",
        "- Quel est l'effet du nombre d'exemples en few-shot learning sur les performances?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d82426-6f9c-4555-aa53-9ce6e33ae48c",
      "metadata": {
        "id": "e3d82426-6f9c-4555-aa53-9ce6e33ae48c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
