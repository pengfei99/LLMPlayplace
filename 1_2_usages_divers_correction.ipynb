{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054f30d3-ac3d-4c7e-a528-efb05ca00735",
   "metadata": {
    "id": "054f30d3-ac3d-4c7e-a528-efb05ca00735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (4.45.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (5.28.2)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (8.1.5)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.51.2)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.3.3)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: fpdf2 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 1)) (4.66.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 2)) (0.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 5)) (8.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 5)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 5)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 5)) (3.0.13)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 7)) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 7)) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 7)) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 7)) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 7)) (0.1.135)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 7)) (8.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.12/site-packages (from fpdf2->-r requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /opt/conda/lib/python3.12/site-packages (from fpdf2->-r requirements.txt (line 9)) (10.4.0)\n",
      "Requirement already satisfied: fonttools>=4.34.0 in /opt/conda/lib/python3.12/site-packages (from fpdf2->-r requirements.txt (line 9)) (4.54.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 6)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 6)) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 2)) (2024.9.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (4.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain->-r requirements.txt (line 7)) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 7)) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 6)) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.2.13)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 5)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0203f-a571-4ae0-a335-4cd3bff429d9",
   "metadata": {
    "id": "b2e0203f-a571-4ae0-a335-4cd3bff429d9"
   },
   "source": [
    "# 1) Table Q&A.\n",
    "\n",
    "Nous allons utiliser TAPAS, le Table Q&A de Google entraîné sur le [WikiTableQuestions](https://paperswithcode.com/dataset/wikitablequestions) (WTQ) dataset. Ici, pas de call d'API.\n",
    "TAPAS permet de répondre à une question dont la réponse se trouve dans une table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f03be097-e4a1-4285-838a-780f8371028c",
   "metadata": {
    "id": "f03be097-e4a1-4285-838a-780f8371028c"
   },
   "outputs": [],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import tiktoken \n",
    "\n",
    "# On charge le modèle et son tokenizer.\n",
    "tokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\n",
    "model = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124e8dcf-7353-40c7-9a90-e1102da5e773",
   "metadata": {
    "id": "124e8dcf-7353-40c7-9a90-e1102da5e773"
   },
   "outputs": [],
   "source": [
    "# Example table in a pandas DataFrame\n",
    "data = {\n",
    "    'Actors': [\"Brad Pitt\", \"Leonardo DiCaprio\", \"Tom Cruise\"],\n",
    "    'Age': [57, 46, 58],\n",
    "    'Movies': [88, 53, 43]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "# Attention de bien mettre la table sous forme de str !\n",
    "table = pd.DataFrame(data).astype(str)\n",
    "\n",
    "# La question\n",
    "queries = [\"How many movies has Leonardo DiCaprio acted in?\"]\n",
    "queries = [\"Who played 88 movies ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aaf9c38-a9da-4147-a809-107bf935c65f",
   "metadata": {
    "id": "9aaf9c38-a9da-4147-a809-107bf935c65f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize inputs as Pytorch tensor (la table et la query)\n",
    "inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645d4208-7ff2-43f7-b4dd-5ca00c1a5a44",
   "metadata": {
    "id": "645d4208-7ff2-43f7-b4dd-5ca00c1a5a44"
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891204f7-72e6-4e80-ae80-dca6908de190",
   "metadata": {
    "id": "891204f7-72e6-4e80-ae80-dca6908de190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(0, 0)]],)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predicted answer coordinates\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(\n",
    "    inputs, outputs.logits.detach().cpu()\n",
    ")\n",
    "predicted_answer_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba091cfc-c1eb-423a-b5b0-534233a78a08",
   "metadata": {
    "id": "ba091cfc-c1eb-423a-b5b0-534233a78a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who played 88 movies ?\n",
      "Predicted answer: ['Brad Pitt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the cell values corresponding to the predicted answer coordinates\n",
    "for question, coordinates in zip(queries, predicted_answer_coordinates[0]):\n",
    "    answers = []\n",
    "    for coordinate in coordinates:\n",
    "        if len(coordinate) == 2:\n",
    "            cell_value = table.iat[coordinate]\n",
    "            answers.append(cell_value)\n",
    "    print(f\"Question: {question}\\nPredicted answer: {answers}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16037b35-0c85-48aa-a1c1-f64e0499b551",
   "metadata": {
    "id": "16037b35-0c85-48aa-a1c1-f64e0499b551"
   },
   "source": [
    "A priori, on aurait pu s'en sortir avec des requêtes SQL ou pandas. Quel est alors l'intérêt de Table Q&A ?\n",
    "\n",
    " * Pas besoin de pré-coder toutes les possibilités de requêtes utilisateur -- c'est très pratique !\n",
    "\n",
    " * Facile d'automatiser les process lorsqu'il y a beaucoup de donnees différentes et que nous avons qu'une seule question à poser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a08c1-f474-4ce3-bdaf-38f6be4c183d",
   "metadata": {
    "id": "f59a08c1-f474-4ce3-bdaf-38f6be4c183d"
   },
   "source": [
    "# 2) Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcec32-9233-4c2a-9cf5-a7b28f244b73",
   "metadata": {
    "id": "96dcec32-9233-4c2a-9cf5-a7b28f244b73"
   },
   "source": [
    "Il suffit d'utiliser l'API d'OpenAI et le paramètre `max_tokens` pour contrôler la taille du résumé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8195af-d479-4db8-8593-4ea2e13d1c91",
   "metadata": {
    "id": "4f8195af-d479-4db8-8593-4ea2e13d1c91"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from credentials.keys import OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Input text to summarize\n",
    "input_text = \"\"\"\n",
    "In a groundbreaking discovery, scientists have found evidence of water on Mars.\n",
    "The discovery was made using a new technique involving spectroscopy.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary using the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the following text in a single sentence: {input_text}\"}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# NB: with older GPT-3.5 Turbo model\n",
    "# response = client.completions.create(\n",
    "#   model=\"gpt-3.5-turbo-instruct\",\n",
    "#   prompt=f\"Summarize the following text: {input_text}\",\n",
    "#   max_tokens=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54513bf-bed4-40a5-b3e3-3050f1674c4f",
   "metadata": {
    "id": "f54513bf-bed4-40a5-b3e3-3050f1674c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists have made a groundbreaking discovery of evidence of water on Mars using a new spectroscopy technique.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687824c6-0c0e-4f15-bb5e-83903949bd0d",
   "metadata": {
    "id": "687824c6-0c0e-4f15-bb5e-83903949bd0d"
   },
   "source": [
    "Évidemment, cette approche est limitée à des textes dont la longueur ne dépasse pas celle du contexte du modèle (ici de 128k tokens uniquement). Pour résumer un texte plus long, il faudrait le découper en chunks, résumer ces chunks, les aggréger et ensuite les résumer (ou alors utiliser un modèle dont la fenêtre de contexte est plus longue...)\n",
    "\n",
    "Il y a de nombreuses façons de faire cela de facon un peu moins naïve, par exemple en déterminant les parties les plus importante du texte et en les résumant en priorité. On peut aussi utiliser des modèles un peu plus spécialisés pour ce genre de tâches.\n",
    "\n",
    "**Ici, nous allons résumer l'histoire de Cendrillon en implémentant cette stratégie de chunking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2e217-c209-446f-a1b1-a93aad441116",
   "metadata": {
    "id": "2fe2e217-c209-446f-a1b1-a93aad441116"
   },
   "source": [
    "**Exercice**: Charger le livre dans Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b99a9b-47e0-4f88-b1df-3f1d7403132e",
   "metadata": {
    "id": "72b99a9b-47e0-4f88-b1df-3f1d7403132e"
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/cinderella.txt\"  # Adjust as per your file path\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_to_summarize = file.read()\n",
    "\n",
    "# print(text_to_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sqGQ8vkyOeGQ",
   "metadata": {
    "id": "sqGQ8vkyOeGQ"
   },
   "source": [
    "**Exercice**: définir une fonction `get_chat_completion` pour envoyer un prompt à l'API d'OpenAI et récupérer le résultat sous forme de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9099849-c71e-4b8a-b61d-6cf19d41f7ad",
   "metadata": {
    "id": "f9099849-c71e-4b8a-b61d-6cf19d41f7ad"
   },
   "outputs": [],
   "source": [
    "from credentials.keys import OPENAI_API_KEY\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_chat_completion(messages, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fef159-951f-4d95-801e-76a5c148ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the following text in a single sentence: {text_to_summarize}\"}\n",
    "    ]\n",
    "\n",
    "summary_from_openAi = get_chat_completion(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "416bb0a7-0890-4465-be12-bc040e9d57c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cinderella, a mistreated young girl forced into servitude by her overbearing stepmother and stepsisters, is magically transformed by her fairy godmother to attend a royal ball, where she captures the heart of the prince, ultimately revealing her true identity and leading to a happy ending for herself and her sisters.\n"
     ]
    }
   ],
   "source": [
    "print(summary_from_openAi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94140cd7-ee77-4c9a-9596-9f78a841ee7d",
   "metadata": {
    "id": "94140cd7-ee77-4c9a-9596-9f78a841ee7d"
   },
   "source": [
    "Ensuite, on définit les fonctions annexes pour tokéniser le texte, le découper en chunks dont le nombre de tokens ne dépasse pas une certaine valeur (pour gérer la limite sur la longueur de la fenêtre de contexte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92188d5f-2baf-4d73-850b-b31b384089e7",
   "metadata": {
    "id": "92188d5f-2baf-4d73-850b-b31b384089e7"
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    '''\n",
    "    Tokénise un texte en utilisant gpt-4o-mini\n",
    "    '''\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    return encoding.encode(text)\n",
    "\n",
    "\n",
    "def chunk_on_delimiter(\n",
    "        input_string: str,\n",
    "        max_tokens: int,\n",
    "        delimiter: str\n",
    "        ) -> list[str]:\n",
    "    '''\n",
    "    Chunk a text into smaller pieces based on a maximum token count and a delimiter.\n",
    "    '''\n",
    "    chunks = input_string.split(delimiter)\n",
    "    combined_chunks, _, dropped_chunk_count = combine_chunks(\n",
    "        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis_for_overflow=True\n",
    "    )\n",
    "    if dropped_chunk_count > 0:\n",
    "        print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n",
    "    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n",
    "    return combined_chunks\n",
    "\n",
    "\n",
    "def combine_chunks(\n",
    "        chunks: list[str],\n",
    "        max_tokens: int,\n",
    "        chunk_delimiter: str = \"\\n\\n\",\n",
    "        header: str = None,\n",
    "        add_ellipsis_for_overflow: bool = False,\n",
    "        ) -> tuple[list[str], list[int]]:\n",
    "    '''\n",
    "    Combine text chunks into larger blocks without exceeding a specified token count.\n",
    "    Return the combined text blocks, their original indices, and the count of chunks\n",
    "    dropped due to overflow.\n",
    "    '''\n",
    "    dropped_chunk_count = 0\n",
    "    output = []  # list to hold the final combined chunks\n",
    "    output_indices = []  # list to hold the indices of the final combined chunks\n",
    "    candidate = (\n",
    "        [] if header is None else [header]\n",
    "    )\n",
    "\n",
    "    candidate_indices = []\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "\n",
    "        chunk_with_header = [chunk] if header is None else [header, chunk]\n",
    "        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n",
    "            print(\"Warning: Chunk overflow\")\n",
    "            if (\n",
    "                    add_ellipsis_for_overflow\n",
    "                    and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n",
    "            ):\n",
    "                candidate.append(\"...\")\n",
    "                dropped_chunk_count += 1\n",
    "            continue\n",
    "\n",
    "        # estimate token count with the current chunk added\n",
    "        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n",
    "        # If the token count exceeds max_tokens, add the current candidate to output and start a new candidate\n",
    "        if extended_candidate_token_count > max_tokens:\n",
    "            output.append(chunk_delimiter.join(candidate))\n",
    "            output_indices.append(candidate_indices)\n",
    "            candidate = chunk_with_header  # re-initialize candidate\n",
    "            candidate_indices = [chunk_i]\n",
    "        # otherwise keep extending the candidate\n",
    "        else:\n",
    "            candidate.append(chunk)\n",
    "            candidate_indices.append(chunk_i)\n",
    "\n",
    "    # add the remaining candidate to output if it's not empty\n",
    "    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n",
    "        output.append(chunk_delimiter.join(candidate))\n",
    "        output_indices.append(candidate_indices)\n",
    "\n",
    "    return output, output_indices, dropped_chunk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8ef2e-80ec-4f4c-aa54-9b8e050bde23",
   "metadata": {
    "id": "a0b8ef2e-80ec-4f4c-aa54-9b8e050bde23"
   },
   "source": [
    "**Exercice** : écrire une fonction `summarize_text` qui résume un long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54a38509-7d45-45c8-ab62-c445805403c2",
   "metadata": {
    "id": "54a38509-7d45-45c8-ab62-c445805403c2"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def summarize(\n",
    "    text: str,\n",
    "    detail: float = 0,\n",
    "    model: str = 'gpt-4o-mini',\n",
    "    minimum_chunk_size: int = 2000,\n",
    "    chunk_delimiter: str = \".\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarizes a given text by splitting it into chunks, each of which is summarized individually.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be summarized.\n",
    "        detail (float, optional): A value between 0 and 1 indicating the desired level of detail in the summary.\n",
    "          0 leads to a higher level summary, and 1 results in a more detailed summary. Defaults to 0.\n",
    "        model (str, optional): The model to use for generating summaries. Defaults to 'gpt-3.5-turbo'.\n",
    "        minimum_chunk_size (Optional[int], optional): The minimum size for text chunks. Defaults to 500.\n",
    "        chunk_delimiter (str, optional): The delimiter used to split the text into chunks. Defaults to \".\".\n",
    "\n",
    "    Returns:\n",
    "    - str: The final compiled summary of the text.\n",
    "\n",
    "    The function first determines the number of chunks by interpolating between a minimum and a maximum chunk\n",
    "    count based on the `detail` parameter. It then splits the text into chunks and summarizes each chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # check detail is set correctly\n",
    "    assert 0 <= detail <= 1\n",
    "\n",
    "    # interpolate the number of chunks based to get specified level of detail\n",
    "    max_chunks = len(chunk_on_delimiter(text, minimum_chunk_size, chunk_delimiter))\n",
    "    min_chunks = 1\n",
    "    num_chunks = int(min_chunks + detail * (max_chunks - min_chunks))\n",
    "\n",
    "    # Adjust chunk_size based on interpolated number of chunks\n",
    "    document_length = len(tokenize(text))\n",
    "    chunk_size = max(minimum_chunk_size, document_length // num_chunks)\n",
    "    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n",
    "    print(f\"Splitting the text into {len(text_chunks)} chunks to be summarized.\")\n",
    "    print(f\"Chunk lengths are {[len(tokenize(x)) for x in text_chunks]}\")\n",
    "\n",
    "    # set system message\n",
    "    system_message_content = f\"You will be given different passages from a book one by one.\" + \\\n",
    "      f\"Provide a summary of the following text. When summarizing, directly dive into the narrative \" + \\\n",
    "      f\"or descriptions from the text without using introductory phrases like 'In this passage'.\" + \\\n",
    "      f\"Directly address the main events, characters, and themes, encapsulating the essence and \" + \\\n",
    "      f\"significant details from the text in a flowing narrative. The goal is to present a unified \" + \\\n",
    "      f\"view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\"\n",
    "\n",
    "    accumulated_summaries = []\n",
    "    for chunk in tqdm(text_chunks):\n",
    "        user_message_content = chunk\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content},\n",
    "            {\"role\": \"user\", \"content\": user_message_content}\n",
    "        ]\n",
    "        response = get_chat_completion(messages, model=model)\n",
    "        accumulated_summaries.append(response)\n",
    "\n",
    "    # Compile final summary from partial summaries\n",
    "    final_summary = '\\n\\n'.join(accumulated_summaries)\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "728f5fdb-87f4-4a8d-80a5-0a4ad11f8fa6",
   "metadata": {
    "id": "728f5fdb-87f4-4a8d-80a5-0a4ad11f8fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 1 chunks to be summarized.\n",
      "Chunk lengths are [6696]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.25s/it]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(text_to_summarize, detail=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f2d52a-5012-4697-b894-9e42f66b1435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gentleman, having married a proud widow with two grown daughters, found his gentle daughter from his first marriage subjected to harsh treatment by her new stepfamily. The stepmother, envious of the girl's beauty and goodness, forced her to perform all the household chores while her own daughters indulged in luxury. The girl, known as Cinderella, endured her mistreatment in silence, often retreating to the cinders of the fireplace for solace.\n",
      "\n",
      "When the King’s son announced a grand ball, Cinderella's stepsisters were thrilled, demanding her help to prepare their outfits. Despite her own longing to attend, Cinderella was ridiculed and dismissed. However, her fairy godmother appeared, transforming a pumpkin into a magnificent coach, mice into horses, and a rat into a coachman, while also dressing Cinderella in a stunning gown and glass slippers. The fairy warned her that the magic would end at midnight.\n",
      "\n",
      "At the ball, Cinderella captivated everyone, especially the prince, who was enchanted by her beauty. She danced with him, but as the clock struck midnight, she fled, leaving behind one glass slipper. The prince, determined to find the mysterious girl, searched the kingdom for the owner of the slipper.\n",
      "\n",
      "Cinderella's stepsisters tried unsuccessfully to fit into the slipper, but when Cinderella herself tried it on, it fit perfectly. Her fairy godmother reappeared, enhancing her beauty even further. The stepsisters, realizing their mistake, begged for forgiveness, which Cinderella graciously granted. Ultimately, she married the prince, and her stepsisters were married to two noble lords, living happily ever after.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19DMhdGLThv0",
   "metadata": {
    "id": "19DMhdGLThv0"
   },
   "source": [
    "On enregistre à présent le résumé sous forme de PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25YlRIQiTxAJ",
   "metadata": {
    "id": "25YlRIQiTxAJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (1.51.2)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: fpdf2 in /opt/conda/lib/python3.12/site-packages (2.8.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /opt/conda/lib/python3.12/site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain) (0.1.135)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.12/site-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /opt/conda/lib/python3.12/site-packages (from fpdf2) (10.4.0)\n",
      "Requirement already satisfied: fonttools>=4.34.0 in /opt/conda/lib/python3.12/site-packages (from fpdf2) (4.54.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai tiktoken fpdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa8b4e73-6ec8-44d1-97e9-cf96bf9c4d9e",
   "metadata": {
    "id": "aa8b4e73-6ec8-44d1-97e9-cf96bf9c4d9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2742/3246724405.py:6: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  self.set_font('Arial', 'B', 15)\n",
      "/tmp/ipykernel_2742/3246724405.py:10: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  self.cell(30, 10, 'Summary', 1, 0, 'C')\n",
      "/tmp/ipykernel_2742/3246724405.py:24: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "/tmp/ipykernel_2742/3246724405.py:18: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  self.set_font('Arial', 'I', 8)\n",
      "/tmp/ipykernel_2742/3246724405.py:20: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  self.cell(0, 10, 'Page %s' % self.page_no(), 0, 0, 'C')\n"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "   def header(self):\n",
    "       # Select Arial bold 15\n",
    "       self.set_font('Arial', 'B', 15)\n",
    "       # Move to the right\n",
    "       self.cell(80)\n",
    "       # Framed title\n",
    "       self.cell(30, 10, 'Summary', 1, 0, 'C')\n",
    "       # Line break\n",
    "       self.ln(20)\n",
    "\n",
    "   def footer(self):\n",
    "       # Go to 1.5 cm from bottom\n",
    "       self.set_y(-15)\n",
    "       # Select Arial italic 8\n",
    "       self.set_font('Arial', 'I', 8)\n",
    "       # Page number\n",
    "       self.cell(0, 10, 'Page %s' % self.page_no(), 0, 0, 'C')\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "summary_utf8 = summary.encode('latin-1', 'replace').decode('latin-1')\n",
    "pdf.multi_cell(0, 10, summary_utf8)\n",
    "pdf_output_path = \"./summary.pdf\"\n",
    "pdf.output(pdf_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0111c-88f2-4be0-bb92-abd567a7f60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "KVBxRp1vSJKy",
   "metadata": {
    "id": "KVBxRp1vSJKy"
   },
   "source": [
    "**Exercice (optionnel)**: adapter la fonction précédente pour résumer de manière récursive, c'est-à-dire en prenant en compte les résumés des chunks précédents pour chaque chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufxqz6MLSYp-",
   "metadata": {
    "id": "ufxqz6MLSYp-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5692ed8d-507b-4f92-bf9f-a489ef86c4f1",
   "metadata": {
    "id": "5692ed8d-507b-4f92-bf9f-a489ef86c4f1"
   },
   "source": [
    "On observe en pratique que cette approche ne fonctionne pas toujours très bien. On peut aller plus loin en utilisant des modèles dit encoder-decoder plus à même de résoudre ce genre de tâches (comme **BERT** ou **RoBERTA**).\n",
    "- https://www.width.ai/post/4-long-text-summarization-methods\n",
    "- https://medium.com/@pvsravanth/unlocking-the-power-of-text-summarization-with-large-language-models-llms-522372e7f9e0\n",
    "\n",
    "En explorant HuggingFace, on peut trouver plusieurs modèles qui sont censés être performants pour cette tâche :\n",
    "- **BERT_summary**: Une variante de BERT fine-tuned pour la summarization en particulier. https://huggingface.co/Shobhank-iiitdwd/BERT_summary/tree/main\n",
    "- **pegasus-multi_news** : https://huggingface.co/google/pegasus-multi_news PEGASUS = Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence. Développé par Google.\n",
    "- **longformer-base-4096** : https://huggingface.co/allenai/longformer-base-4096\n",
    "- **bigbird-roberta-base**: https://huggingface.co/google/bigbird-roberta-base by Google\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61c28c-dfdd-4c1b-9a00-35bdcd38c1aa",
   "metadata": {
    "id": "4a61c28c-dfdd-4c1b-9a00-35bdcd38c1aa"
   },
   "source": [
    "On peut commencer par essayer de voir si l'on peut facilement telecharger le modele \"Shobhank-iiitdwd/BERT_summary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d80f1-181c-4971-b9bc-a9ec9d7b47ea",
   "metadata": {},
   "source": [
    "### Trouble shoot on T5tokenizer \n",
    "\n",
    "If you have the below problem, `T5Tokenizer requires the SentencePiece library but it was not found in your environment.`\n",
    "\n",
    "Try to run the below code\n",
    "\n",
    "```shell\n",
    "# install the system dependencies\n",
    "sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
    "\n",
    "# install the python dependencies\n",
    "pip install sentencepiece \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0844a7de-2ed3-475f-b33e-00bbcad9dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Package pkg-config is not available, but is referred to by another package.\n",
      "This may mean that the package is missing, has been obsoleted, or\n",
      "is only available from another source\n",
      "\n",
      "Package cmake is not available, but is referred to by another package.\n",
      "This may mean that the package is missing, has been obsoleted, or\n",
      "is only available from another source\n",
      "\n",
      "E: Package 'cmake' has no installation candidate\n",
      "E: Package 'pkg-config' has no installation candidate\n",
      "E: Unable to locate package libgoogle-perftools-dev\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f05619c-ed35-49d5-8e17-f646c61bb7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c61d856-982a-415e-91e8-e2de8c8395a9",
   "metadata": {
    "id": "4c61d856-982a-415e-91e8-e2de8c8395a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d7235e81cb4bc8b01989ecc545258f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c9317b598749dcb2fb7ef31fa96ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a7c3a192184b0e8e461e7afd418b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained T5 model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShobhank-iiitdwd/BERT_summary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShobhank-iiitdwd/BERT_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2208\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2206\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2442\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2441\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2442\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2444\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2447\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5.py:150\u001b[0m, in \u001b[0;36mT5Tokenizer.__init__\u001b[0;34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_ids \u001b[38;5;241m=\u001b[39m extra_ids\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model_kwargs)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m additional_special_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     extra_tokens \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m additional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<extra_id_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x)]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sentencepiece/__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[1;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sentencepiece/__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Shobhank-iiitdwd/BERT_summary\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Shobhank-iiitdwd/BERT_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937161a3-f709-461d-ade5-bfb950fe4c19",
   "metadata": {},
   "source": [
    "## Some important point on model and tokenizer\n",
    "\n",
    "Each model is trained with a specific tokenizer. If we send query by using a different tokenizer. The response will be complete wrong.\n",
    "\n",
    "Below is an example. we load a bad tokenizer for the target model\n",
    "\n",
    "You will see the warning message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02698951-f367-4f15-9d19-93db1e16c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/pegasus-multi_news\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/pegasus-multi_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643ab69-241f-4e4d-884d-0b23cacf94b7",
   "metadata": {},
   "source": [
    "Now let's load the right tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96cbc7f-3135-46e6-bf1a-317e8d683a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained PEGASUS tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-multi_news\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-multi_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7836d5-7fa1-4cda-9d67-6aa246beea3c",
   "metadata": {
    "id": "cc7836d5-7fa1-4cda-9d67-6aa246beea3c"
   },
   "source": [
    "On tente maintenant le modele \"google/pegasus-multi_news\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2405ec-1a95-4026-b4e4-7921e153ddf6",
   "metadata": {
    "id": "4a2405ec-1a95-4026-b4e4-7921e153ddf6"
   },
   "source": [
    "En fait, pour que cela fonction il faut télécharger le modèle directement via des fonctions plus spécialisées **PegasusTokenizer** ou **PegasusForConditionalGeneration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86305d9a-31e5-480e-8cc5-1674ecf69b10",
   "metadata": {
    "id": "86305d9a-31e5-480e-8cc5-1674ecf69b10"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def summarize_text(\n",
    "        text: str,\n",
    "        max_length: int = 300,\n",
    "        min_length: int = 30,\n",
    "        num_beams: int = 4):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    -------\n",
    "        text: The input text to be summarized.\n",
    "        num_beams: Number of beams for beam search. Plus il y a de beam mieux c'est mais plus le cout computationnel explose.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(\"Summarize: \" + text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    print(f'Input shape: {inputs.shape}')\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs.to(device),\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=False\n",
    "    )\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf365e5c-b69e-41b4-8343-4ad3f9b84c4a",
   "metadata": {
    "id": "bf365e5c-b69e-41b4-8343-4ad3f9b84c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Input shape: torch.Size([1, 1024])\n",
      "– It's the story of a stepmother who forced her daughters to do the household work, and it's a tale that's been told before—but now it's getting new life as a children's book. In Cinderella, by Emma Donoghue, a young girl named Cinderella is forced to do the work of a stepmother who was a widow with two grown-up daughters. \"They scold me for that,\" she writes. \"If I do not run at once when the bell rings, they scold me for that. Yet they ring—both of them together sometimes—a minute after setting me to rake out a grate and sift the ashes. As for looking at myself in the glass, gladly would I do it if they allowed me one. But they have told me that if I had a glass I should only waste time in front of it. ... She kept these thoughts to herself, however, and suffered her ill-usage patiently, not daring to complain to her father, who would, moreover, have joined with the others in chiding her, for he was wholly under his wife's thumb. When she had done her work she used to creep away to the chimney-corner and seat herself among the cinders. ... They were wise in their way to deprive her of a looking-glass; for in truth, and in spite of her sorry rags, Cinderella was a hundred times more beautiful than they with all their magnificent dresses.\" Click for the full\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(text_to_summarize)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f908f4-e442-4e36-8a34-749120dee272",
   "metadata": {
    "id": "96f908f4-e442-4e36-8a34-749120dee272"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d35c0-0ec0-4950-8a97-e54d38200758",
   "metadata": {
    "id": "4b4d35c0-0ec0-4950-8a97-e54d38200758"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d734b4-4768-4133-ab48-97c71fe1d121",
   "metadata": {
    "id": "79d734b4-4768-4133-ab48-97c71fe1d121"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db07362-ab1e-474d-a5ad-5c5fe966472d",
   "metadata": {
    "id": "1db07362-ab1e-474d-a5ad-5c5fe966472d"
   },
   "source": [
    "Quelques remarques de ce TD\n",
    "- Il y a de nombreux modèles disponibles pour une tâche spécifique.\n",
    "- Commencer par tester si on arrive a y avoir accès facilement (pas d'erreur de package ou autre qui prendraient du temps à gérer).\n",
    "- Il faut tout de même faire un nombre d'expériences assez significatif.\n",
    "- Étant donné le livre que l'on a extrait, il vaudrait mieux bien nettoyer le texte pour éviter des ennuis inutiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d02da8-37de-4a08-90a4-082abc515322",
   "metadata": {
    "id": "98d02da8-37de-4a08-90a4-082abc515322"
   },
   "source": [
    "# 3) Named Entity Recognition NER\n",
    "\n",
    "C'est la tâche qui consiste à identifier et classifier les entités nommées (noms propres) dans des catégories prédéfinies telles que les noms de personnes, d'organisations, de lieux, de dates, de quantités, etc.\n",
    "\n",
    "Là encore, on peut utiliser du prompt pour résoudre cette tâche. Noter la encore que des modèles spécialisés peuvent etre plus performants et moins couteux pour ce genre de tâches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23475aa3-fe90-4bdf-99ee-d5592ebf2c9a",
   "metadata": {
    "id": "23475aa3-fe90-4bdf-99ee-d5592ebf2c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (1.51.2)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e384dc1f-16f6-41d2-9f03-3f1003296694",
   "metadata": {
    "id": "e384dc1f-16f6-41d2-9f03-3f1003296694"
   },
   "outputs": [],
   "source": [
    " # librairie de OpenAI\n",
    "\n",
    "from credentials.keys import OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Example text to analyze for named entities\n",
    "input_text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
    "\n",
    "# Construct a prompt to identify named entities\n",
    "prompt = f\"Identify the named entities in the following text. \" + \\\n",
    "    f\"You should return the entities in a list format, such as '['entity1', 'entity2', 'entity3']'\" + \\\n",
    "    f\"Here is the text: \\'{input_text}\\'.\"\n",
    "\n",
    "# Request completion from OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}  # Chat-based message format\n",
    "    ],\n",
    "    max_tokens=50  # Control response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee55e07-85eb-4718-848e-d1afac3febf7",
   "metadata": {
    "id": "3ee55e07-85eb-4718-848e-d1afac3febf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple Inc.', 'Cupertino', 'California']\n"
     ]
    }
   ],
   "source": [
    "# Process the response to extract named entities\n",
    "extracted_entities = response.choices[0].message.content\n",
    "\n",
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c7568-812a-4de2-8311-9eac647c7733",
   "metadata": {
    "id": "502c7568-812a-4de2-8311-9eac647c7733"
   },
   "source": [
    "Quels sont les modèles spécialisés pour faire de l'entity recognition ?\n",
    "\n",
    "En cherchant NER sur HuggingFace, on obtient par exemple :\n",
    "- bert-base-NER https://huggingface.co/dslim/bert-base-NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e18b40f-4b7d-4f0f-acc1-a5dfc592f046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422,
     "referenced_widgets": [
      "36277cf9b07f4d8c9ca0f28037bf2c19",
      "d6d019adc98a40a193055463f487e5b6",
      "29dc514a989e4b94adf5ca5a9cf9f075",
      "8ffc33c8367640d9b260dbba0c7ab68a",
      "1680ba054a894dee8188489e5f356253",
      "33050e54aae943ddac5b1699318e9322",
      "b25238e9c51d4545910e82daa6210bb1",
      "3bf60be6fab0459e8ed1b63932090611",
      "cb212653214e4221992fd034999c155b",
      "7a7937b8990841b7857ae71573c91a50",
      "4742af62f5ad435aa5c693935899cb63",
      "e954f0b1b05046de90c34da428bdbc60",
      "dc89deab7cb34cd89bfd52605c3c29bf",
      "9fad6745e7644a4cafe075c535b8b2e0",
      "96fd151b374546c9958cfe8ef9a3792d",
      "160e35f3fa89461eb27063a714edc32b",
      "4f0f3a394e9b48839047669fe9a621f3",
      "10ef24d7af9a4d0b87cbaf4a124fbc32",
      "3e15be00e7444d0dbd893abf3b7f582f",
      "b95c86eeaca7482dbdcd8484e85edc1b",
      "16acdbc124d849a491b91d9d26802fc7",
      "16a08a968de048ce975029ec1a0ec556",
      "a8d82fa2d54c4a4488c8845357bd39d8",
      "553a4e04df024eaea7c4c643628c74a1",
      "5a98c4049fe04b6aacf8a4f2777e4fd2",
      "53a4e60e650c4b6db40c0807eb9581fe",
      "ed19b3408a454669a46c8a0e8ac5a384",
      "29312d40d219459c94b500e38138f13b",
      "e99c95e261fe443995ebd6cd1617b89c",
      "661d7c92cadf41c8a18880a0f339c244",
      "baa9ef7bf33445cabda579951b806593",
      "fab667ffd17c47bc815b3c51f2733afc",
      "bee4a1fe79504819bf7ebe15ec7972d2",
      "a6a11ef680374abdb66b1e24ec0413d5",
      "0f6767fdba4b4727811b7565b1aca84d",
      "5787e1731c664d68bc2b51ceb4cc67ea",
      "34b53bcd05cc45938dc01bcca30c8b6d",
      "852966f64f5a4ef5971a07b131f29cdf",
      "93ee76fc32a04e2da6d6196d788c597d",
      "8bab2b96ab0d4222bdbab9f2936aadc0",
      "49f80ce1b2464dd28cf538b271e1d51e",
      "6dc7cc4130b745b08dff4035e0b1990b",
      "7054542451b34e31902609b5ab904278",
      "48e37642d3794dd1864982e3f82787c7",
      "e8c612e8df044aafb1232de87c83ecd0",
      "92170f9eeb6e44bab7a6f21aa08f6821",
      "19014a18e31a4bfbb75e1e723e91c4a9",
      "15ea31f5ccb64b719c9b4a919633e14c",
      "ce0936a7c375422390f3eee0a2a3bc50",
      "e1c8a76d6aca42e59c8f94ee8292629b",
      "a6fba52d70e24bd19d37aced1b0d0db4",
      "d18cda9e9a884082b22ba8d9005989bb",
      "26686e03aeb24b34b9d95a12c4078656",
      "95a18f8e2101469fa3ee9ff1564feb09",
      "0f2f539c4aab425db03482acbd18343b",
      "1259fe3fd9fa49808bd43ec6b47d11e0",
      "c6d4219591fd49a9a3813ae46391b994",
      "f4c234cfe00d418b90671516089adadf",
      "a888c9f8e49c486b8c664fb57a970d71",
      "a77b014fcb9f47c7aa9b54d8d1f514e3",
      "4d2501c4bd55475aaf66c096d8447724",
      "3b3b75a50c1e45368c90072e87819d17",
      "817a02813bfc4d27b9584592c1983f79",
      "60d132d4364a44e6988528163cd59123",
      "f181e353fb5744cb8d170e197c5bd56a",
      "bf1a323cde0444fbbd4a90d65bb1321e"
     ]
    },
    "executionInfo": {
     "elapsed": 48343,
     "status": "ok",
     "timestamp": 1728832016614,
     "user": {
      "displayName": "Alexandre Tuel",
      "userId": "09272815080792413028"
     },
     "user_tz": -120
    },
    "id": "2e18b40f-4b7d-4f0f-acc1-a5dfc592f046",
    "outputId": "6bd19480-1296-4c0a-f4ee-2f1356c53b00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7877e785-5b4b-41ae-8f51-bc64a40bd102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1728832091207,
     "user": {
      "displayName": "Alexandre Tuel",
      "userId": "09272815080792413028"
     },
     "user_tz": -120
    },
    "id": "7877e785-5b4b-41ae-8f51-bc64a40bd102",
    "outputId": "84f52d41-df7f-4dc5-d21e-a7af86fa490f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wolfgang', 'Berlin', 'France', 'Matthias', 'Italy']\n",
      "['B-PER', 'B-LOC', 'B-LOC', 'B-PER', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin but I often travel to France, although my friend Matthias lives in Italy.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print([d['word'] for d in ner_results])\n",
    "print([d['entity'] for d in ner_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baafc7f4-b338-4e99-9c99-aa3c444084cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'entity': 'B-LOC', 'score': np.float32(0.9997434), 'index': 23, 'word': 'Italy', 'start': 108, 'end': 113}, {'entity': 'B-PER', 'score': np.float32(0.99890935), 'index': 20, 'word': 'Matthias', 'start': 90, 'end': 98}, {'entity': 'B-LOC', 'score': np.float32(0.99979323), 'index': 15, 'word': 'France', 'start': 63, 'end': 69}, {'entity': 'B-LOC', 'score': np.float32(0.9997359), 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}, {'entity': 'B-PER', 'score': np.float32(0.9992913), 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}]\n"
     ]
    }
   ],
   "source": [
    "print(type(ner_results))\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gCtzm4hlaZ53",
   "metadata": {
    "id": "gCtzm4hlaZ53"
   },
   "source": [
    "**Exercice** : Anonymiser le texte grâce à la méthode de NER.\n",
    "\n",
    "*Indices* : Les indices de début/fin de chaque entité dans la chaîne de caractères de départ sont donnés par `ner_results[idx]['start']` et `ner_results[idx]['end']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rmb2flgtaZNn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1728832038383,
     "user": {
      "displayName": "Alexandre Tuel",
      "userId": "09272815080792413028"
     },
     "user_tz": -120
    },
    "id": "rmb2flgtaZNn",
    "outputId": "fe8555bd-2157-4bdd-9dd4-3e5a875058a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized Text: My name is PER and I live in LOC but I often travel to LOC, although my friend PER lives in LOC.\n"
     ]
    }
   ],
   "source": [
    "# Anonymize the text\n",
    "anonymized_text = example\n",
    "\n",
    "# Sort NER results in reverse order of their positions\n",
    "# This prevents messing up the indices while replacing\n",
    "ner_results = sorted(ner_results, key=lambda x: x['start'], reverse=True)\n",
    "\n",
    "# Replace entities with placeholders\n",
    "for entity in ner_results:\n",
    "    entity_label = entity['entity'].split('-')[-1]  # Get the entity type (e.g., PERSON, LOC, etc.)\n",
    "    start, end = entity['start'], entity['end']\n",
    "    anonymized_text = anonymized_text[:start] + entity_label + anonymized_text[end:]\n",
    "\n",
    "print(\"Anonymized Text:\", anonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pKvHn8HQbs5X",
   "metadata": {
    "id": "pKvHn8HQbs5X"
   },
   "source": [
    "## Optionnel : fine-tuning de NER sur un dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab679a67-112f-4cd6-8278-30dc0bd74654",
   "metadata": {
    "id": "ab679a67-112f-4cd6-8278-30dc0bd74654"
   },
   "source": [
    "Nous pouvons explorer le dataset **conll2003** qui est un dataset sur lesquel chaque les mots sont associes a leur part-of-speech tagging, leur NER. Pour l'instant nous n'allons pas encore evaluer les performances du model sur ce jeu de donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2443cdb-5aa1-46b2-9e47-9e262b72d63d",
   "metadata": {
    "id": "e2443cdb-5aa1-46b2-9e47-9e262b72d63d"
   },
   "outputs": [],
   "source": [
    "# On charge le dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_sentences(filepath: str):\n",
    "    '''\n",
    "    Load the CoNLL-2003 dataset.\n",
    "    '''\n",
    "    final, sentences = [], []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
    "                if len(sentences) > 0:\n",
    "                    final.append(sentences)\n",
    "                    sentences = []\n",
    "            else:\n",
    "                l = line.split(' ')\n",
    "                sentences.append((l[0], l[3].strip('\\n')))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7c90a59-f0b4-45c0-95dc-b4e5d5d3b8cb",
   "metadata": {
    "id": "a7c90a59-f0b4-45c0-95dc-b4e5d5d3b8cb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './conll003-englishversion/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./conll003-englishversion/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m \u001b[43mload_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m load_sentences(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m valid_samples \u001b[38;5;241m=\u001b[39m load_sentences(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mload_sentences\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mLoad the CoNLL-2003 dataset.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m final, sentences \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (line \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-DOCSTART- -X- -X- O\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './conll003-englishversion/train.txt'"
     ]
    }
   ],
   "source": [
    "base_path = './conll003-englishversion/'\n",
    "\n",
    "train_samples = load_sentences(base_path + 'train.txt')\n",
    "test_samples = load_sentences(base_path + 'test.txt')\n",
    "valid_samples = load_sentences(base_path + 'valid.txt')\n",
    "\n",
    "samples = train_samples + test_samples\n",
    "\n",
    "schema = ['_'] + sorted({tag for sentence in samples\n",
    "                             for _, tag in sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adccf76-d60d-4a3f-a7f2-7daa71f8ad66",
   "metadata": {
    "id": "4adccf76-d60d-4a3f-a7f2-7daa71f8ad66",
    "outputId": "0fdb22bc-0a5d-481c-8cd3-2e3cf8b50ea8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'I-LOC',\n",
       " 'I-MISC',\n",
       " 'I-ORG',\n",
       " 'I-PER',\n",
       " 'O']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91cd14-0160-46be-8133-d24e96e97bd2",
   "metadata": {
    "id": "8f91cd14-0160-46be-8133-d24e96e97bd2",
    "outputId": "08171176-0d18-4ad3-d563-ea0c5fc9d3a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, TFAutoModelForTokenClassification\n",
    "\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96e4d3-1b1a-4a7f-905b-047267e91077",
   "metadata": {
    "id": "dc96e4d3-1b1a-4a7f-905b-047267e91077",
    "outputId": "d0cd5fba-50ae-40bb-ef08-2e203e9e2f78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14041it [00:05, 2702.84it/s]\n",
      "3452it [00:01, 2957.40it/s]\n",
      "3249it [00:01, 2517.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_sample(sample):\n",
    "    seq = [\n",
    "               (subtoken, tag)\n",
    "               for token, tag in sample\n",
    "               for subtoken in tokenizer(token)['input_ids'][1:-1]\n",
    "           ]\n",
    "    return [(3, 'O')] + seq + [(4, 'O')]\n",
    "\n",
    "def preprocess(samples):\n",
    "    tag_index = {tag: i for i, tag in enumerate(schema)}\n",
    "    tokenized_samples = list(tqdm(map(tokenize_sample, samples)))\n",
    "    max_len = max(map(len, tokenized_samples))\n",
    "    X = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "    y = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "    for i, sentence in enumerate(tokenized_samples):\n",
    "        for j, (subtoken_id, tag) in enumerate(sentence):\n",
    "            X[i, j] = subtoken_id\n",
    "            y[i,j] = tag_index[tag]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = preprocess(train_samples)\n",
    "X_test, y_test = preprocess(test_samples)\n",
    "X_valid, y_valid = preprocess(valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d591919-dbb5-4ef1-b46d-0ed299b5e995",
   "metadata": {
    "id": "1d591919-dbb5-4ef1-b46d-0ed299b5e995"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.000001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "history = model.fit(\n",
    "    tf.constant(X_train), tf.constant(y_train),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2aea29-7736-46f1-b4dd-3a168b2c0732",
   "metadata": {
    "id": "cc2aea29-7736-46f1-b4dd-3a168b2c0732"
   },
   "outputs": [],
   "source": [
    "[loss, accuracy] = model.evaluate(X_valid, y_valid)\n",
    "print(\"Loss:%1.3f, Accuracy:%1.3f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000b108-1ad7-4278-8ef9-15dcd4140929",
   "metadata": {
    "id": "8000b108-1ad7-4278-8ef9-15dcd4140929"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f2f539c4aab425db03482acbd18343b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f6767fdba4b4727811b7565b1aca84d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ee76fc32a04e2da6d6196d788c597d",
      "placeholder": "​",
      "style": "IPY_MODEL_8bab2b96ab0d4222bdbab9f2936aadc0",
      "value": "added_tokens.json: 100%"
     }
    },
    "10ef24d7af9a4d0b87cbaf4a124fbc32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1259fe3fd9fa49808bd43ec6b47d11e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6d4219591fd49a9a3813ae46391b994",
       "IPY_MODEL_f4c234cfe00d418b90671516089adadf",
       "IPY_MODEL_a888c9f8e49c486b8c664fb57a970d71"
      ],
      "layout": "IPY_MODEL_a77b014fcb9f47c7aa9b54d8d1f514e3"
     }
    },
    "15ea31f5ccb64b719c9b4a919633e14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95a18f8e2101469fa3ee9ff1564feb09",
      "placeholder": "​",
      "style": "IPY_MODEL_0f2f539c4aab425db03482acbd18343b",
      "value": " 112/112 [00:00&lt;00:00, 976B/s]"
     }
    },
    "160e35f3fa89461eb27063a714edc32b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1680ba054a894dee8188489e5f356253": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16a08a968de048ce975029ec1a0ec556": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16acdbc124d849a491b91d9d26802fc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19014a18e31a4bfbb75e1e723e91c4a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d18cda9e9a884082b22ba8d9005989bb",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26686e03aeb24b34b9d95a12c4078656",
      "value": 112
     }
    },
    "26686e03aeb24b34b9d95a12c4078656": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29312d40d219459c94b500e38138f13b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29dc514a989e4b94adf5ca5a9cf9f075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bf60be6fab0459e8ed1b63932090611",
      "max": 59,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb212653214e4221992fd034999c155b",
      "value": 59
     }
    },
    "33050e54aae943ddac5b1699318e9322": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34b53bcd05cc45938dc01bcca30c8b6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7054542451b34e31902609b5ab904278",
      "placeholder": "​",
      "style": "IPY_MODEL_48e37642d3794dd1864982e3f82787c7",
      "value": " 2.00/2.00 [00:00&lt;00:00, 15.9B/s]"
     }
    },
    "36277cf9b07f4d8c9ca0f28037bf2c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6d019adc98a40a193055463f487e5b6",
       "IPY_MODEL_29dc514a989e4b94adf5ca5a9cf9f075",
       "IPY_MODEL_8ffc33c8367640d9b260dbba0c7ab68a"
      ],
      "layout": "IPY_MODEL_1680ba054a894dee8188489e5f356253"
     }
    },
    "3b3b75a50c1e45368c90072e87819d17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3bf60be6fab0459e8ed1b63932090611": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e15be00e7444d0dbd893abf3b7f582f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4742af62f5ad435aa5c693935899cb63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48e37642d3794dd1864982e3f82787c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49f80ce1b2464dd28cf538b271e1d51e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d2501c4bd55475aaf66c096d8447724": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f0f3a394e9b48839047669fe9a621f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53a4e60e650c4b6db40c0807eb9581fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fab667ffd17c47bc815b3c51f2733afc",
      "placeholder": "​",
      "style": "IPY_MODEL_bee4a1fe79504819bf7ebe15ec7972d2",
      "value": " 213k/213k [00:00&lt;00:00, 1.38MB/s]"
     }
    },
    "553a4e04df024eaea7c4c643628c74a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29312d40d219459c94b500e38138f13b",
      "placeholder": "​",
      "style": "IPY_MODEL_e99c95e261fe443995ebd6cd1617b89c",
      "value": "vocab.txt: 100%"
     }
    },
    "5787e1731c664d68bc2b51ceb4cc67ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49f80ce1b2464dd28cf538b271e1d51e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dc7cc4130b745b08dff4035e0b1990b",
      "value": 2
     }
    },
    "5a98c4049fe04b6aacf8a4f2777e4fd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_661d7c92cadf41c8a18880a0f339c244",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_baa9ef7bf33445cabda579951b806593",
      "value": 213450
     }
    },
    "60d132d4364a44e6988528163cd59123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "661d7c92cadf41c8a18880a0f339c244": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dc7cc4130b745b08dff4035e0b1990b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7054542451b34e31902609b5ab904278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a7937b8990841b7857ae71573c91a50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "817a02813bfc4d27b9584592c1983f79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "852966f64f5a4ef5971a07b131f29cdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bab2b96ab0d4222bdbab9f2936aadc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ffc33c8367640d9b260dbba0c7ab68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a7937b8990841b7857ae71573c91a50",
      "placeholder": "​",
      "style": "IPY_MODEL_4742af62f5ad435aa5c693935899cb63",
      "value": " 59.0/59.0 [00:00&lt;00:00, 553B/s]"
     }
    },
    "92170f9eeb6e44bab7a6f21aa08f6821": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1c8a76d6aca42e59c8f94ee8292629b",
      "placeholder": "​",
      "style": "IPY_MODEL_a6fba52d70e24bd19d37aced1b0d0db4",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "93ee76fc32a04e2da6d6196d788c597d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95a18f8e2101469fa3ee9ff1564feb09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96fd151b374546c9958cfe8ef9a3792d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16acdbc124d849a491b91d9d26802fc7",
      "placeholder": "​",
      "style": "IPY_MODEL_16a08a968de048ce975029ec1a0ec556",
      "value": " 829/829 [00:00&lt;00:00, 16.9kB/s]"
     }
    },
    "9fad6745e7644a4cafe075c535b8b2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e15be00e7444d0dbd893abf3b7f582f",
      "max": 829,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b95c86eeaca7482dbdcd8484e85edc1b",
      "value": 829
     }
    },
    "a6a11ef680374abdb66b1e24ec0413d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f6767fdba4b4727811b7565b1aca84d",
       "IPY_MODEL_5787e1731c664d68bc2b51ceb4cc67ea",
       "IPY_MODEL_34b53bcd05cc45938dc01bcca30c8b6d"
      ],
      "layout": "IPY_MODEL_852966f64f5a4ef5971a07b131f29cdf"
     }
    },
    "a6fba52d70e24bd19d37aced1b0d0db4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a77b014fcb9f47c7aa9b54d8d1f514e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a888c9f8e49c486b8c664fb57a970d71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f181e353fb5744cb8d170e197c5bd56a",
      "placeholder": "​",
      "style": "IPY_MODEL_bf1a323cde0444fbbd4a90d65bb1321e",
      "value": " 433M/433M [00:05&lt;00:00, 88.0MB/s]"
     }
    },
    "a8d82fa2d54c4a4488c8845357bd39d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_553a4e04df024eaea7c4c643628c74a1",
       "IPY_MODEL_5a98c4049fe04b6aacf8a4f2777e4fd2",
       "IPY_MODEL_53a4e60e650c4b6db40c0807eb9581fe"
      ],
      "layout": "IPY_MODEL_ed19b3408a454669a46c8a0e8ac5a384"
     }
    },
    "b25238e9c51d4545910e82daa6210bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b95c86eeaca7482dbdcd8484e85edc1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "baa9ef7bf33445cabda579951b806593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bee4a1fe79504819bf7ebe15ec7972d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf1a323cde0444fbbd4a90d65bb1321e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6d4219591fd49a9a3813ae46391b994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d2501c4bd55475aaf66c096d8447724",
      "placeholder": "​",
      "style": "IPY_MODEL_3b3b75a50c1e45368c90072e87819d17",
      "value": "model.safetensors: 100%"
     }
    },
    "cb212653214e4221992fd034999c155b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce0936a7c375422390f3eee0a2a3bc50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d18cda9e9a884082b22ba8d9005989bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6d019adc98a40a193055463f487e5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33050e54aae943ddac5b1699318e9322",
      "placeholder": "​",
      "style": "IPY_MODEL_b25238e9c51d4545910e82daa6210bb1",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "dc89deab7cb34cd89bfd52605c3c29bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f0f3a394e9b48839047669fe9a621f3",
      "placeholder": "​",
      "style": "IPY_MODEL_10ef24d7af9a4d0b87cbaf4a124fbc32",
      "value": "config.json: 100%"
     }
    },
    "e1c8a76d6aca42e59c8f94ee8292629b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8c612e8df044aafb1232de87c83ecd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92170f9eeb6e44bab7a6f21aa08f6821",
       "IPY_MODEL_19014a18e31a4bfbb75e1e723e91c4a9",
       "IPY_MODEL_15ea31f5ccb64b719c9b4a919633e14c"
      ],
      "layout": "IPY_MODEL_ce0936a7c375422390f3eee0a2a3bc50"
     }
    },
    "e954f0b1b05046de90c34da428bdbc60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc89deab7cb34cd89bfd52605c3c29bf",
       "IPY_MODEL_9fad6745e7644a4cafe075c535b8b2e0",
       "IPY_MODEL_96fd151b374546c9958cfe8ef9a3792d"
      ],
      "layout": "IPY_MODEL_160e35f3fa89461eb27063a714edc32b"
     }
    },
    "e99c95e261fe443995ebd6cd1617b89c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed19b3408a454669a46c8a0e8ac5a384": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f181e353fb5744cb8d170e197c5bd56a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4c234cfe00d418b90671516089adadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_817a02813bfc4d27b9584592c1983f79",
      "max": 433292294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_60d132d4364a44e6988528163cd59123",
      "value": 433292294
     }
    },
    "fab667ffd17c47bc815b3c51f2733afc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
