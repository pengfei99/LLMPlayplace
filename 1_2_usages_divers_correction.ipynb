{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054f30d3-ac3d-4c7e-a528-efb05ca00735",
   "metadata": {
    "id": "054f30d3-ac3d-4c7e-a528-efb05ca00735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0203f-a571-4ae0-a335-4cd3bff429d9",
   "metadata": {
    "id": "b2e0203f-a571-4ae0-a335-4cd3bff429d9"
   },
   "source": [
    "# 1) Table Q&A.\n",
    "\n",
    "Nous allons utiliser TAPAS, le Table Q&A de Google entraîné sur le [WikiTableQuestions](https://paperswithcode.com/dataset/wikitablequestions) (WTQ) dataset. Ici, pas de call d'API.\n",
    "TAPAS permet de répondre à une question dont la réponse se trouve dans une table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03be097-e4a1-4285-838a-780f8371028c",
   "metadata": {
    "id": "f03be097-e4a1-4285-838a-780f8371028c"
   },
   "outputs": [],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "# On charge le modèle et son tokenizer.\n",
    "tokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\n",
    "model = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124e8dcf-7353-40c7-9a90-e1102da5e773",
   "metadata": {
    "id": "124e8dcf-7353-40c7-9a90-e1102da5e773"
   },
   "outputs": [],
   "source": [
    "# Example table in a pandas DataFrame\n",
    "data = {\n",
    "    'Actors': [\"Brad Pitt\", \"Leonardo DiCaprio\", \"Tom Cruise\"],\n",
    "    'Age': [57, 46, 58],\n",
    "    'Movies': [88, 53, 43]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "# Attention de bien mettre la table sous forme de str !\n",
    "table = pd.DataFrame(data).astype(str)\n",
    "\n",
    "# La question\n",
    "queries = [\"How many movies has Leonardo DiCaprio acted in?\"]\n",
    "queries = [\"Who played 88 movies ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaf9c38-a9da-4147-a809-107bf935c65f",
   "metadata": {
    "id": "9aaf9c38-a9da-4147-a809-107bf935c65f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize inputs as Pytorch tensor (la table et la query)\n",
    "inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "645d4208-7ff2-43f7-b4dd-5ca00c1a5a44",
   "metadata": {
    "id": "645d4208-7ff2-43f7-b4dd-5ca00c1a5a44"
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891204f7-72e6-4e80-ae80-dca6908de190",
   "metadata": {
    "id": "891204f7-72e6-4e80-ae80-dca6908de190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(0, 0)]],)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predicted answer coordinates\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(\n",
    "    inputs, outputs.logits.detach().cpu()\n",
    ")\n",
    "predicted_answer_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba091cfc-c1eb-423a-b5b0-534233a78a08",
   "metadata": {
    "id": "ba091cfc-c1eb-423a-b5b0-534233a78a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who played 88 movies ?\n",
      "Predicted answer: ['Brad Pitt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the cell values corresponding to the predicted answer coordinates\n",
    "for question, coordinates in zip(queries, predicted_answer_coordinates[0]):\n",
    "    answers = []\n",
    "    for coordinate in coordinates:\n",
    "        if len(coordinate) == 2:\n",
    "            cell_value = table.iat[coordinate]\n",
    "            answers.append(cell_value)\n",
    "    print(f\"Question: {question}\\nPredicted answer: {answers}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16037b35-0c85-48aa-a1c1-f64e0499b551",
   "metadata": {
    "id": "16037b35-0c85-48aa-a1c1-f64e0499b551"
   },
   "source": [
    "A priori, on aurait pu s'en sortir avec des requêtes SQL ou pandas. Quel est alors l'intérêt de Table Q&A ?\n",
    "\n",
    " * Pas besoin de pré-coder toutes les possibilités de requêtes utilisateur -- c'est très pratique !\n",
    "\n",
    " * Facile d'automatiser les process lorsqu'il y a beaucoup de donnees différentes et que nous avons qu'une seule question à poser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a08c1-f474-4ce3-bdaf-38f6be4c183d",
   "metadata": {
    "id": "f59a08c1-f474-4ce3-bdaf-38f6be4c183d"
   },
   "source": [
    "# 2) Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcec32-9233-4c2a-9cf5-a7b28f244b73",
   "metadata": {
    "id": "96dcec32-9233-4c2a-9cf5-a7b28f244b73"
   },
   "source": [
    "Il suffit d'utiliser l'API d'OpenAI et le paramètre `max_tokens` pour contrôler la taille du résumé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8195af-d479-4db8-8593-4ea2e13d1c91",
   "metadata": {
    "id": "4f8195af-d479-4db8-8593-4ea2e13d1c91"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from credentials.keys import OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Input text to summarize\n",
    "input_text = \"\"\"\n",
    "In a groundbreaking discovery, scientists have found evidence of water on Mars.\n",
    "The discovery was made using a new technique involving spectroscopy.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary using the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the following text in a single sentence: {input_text}\"}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# NB: with older GPT-3.5 Turbo model\n",
    "# response = client.completions.create(\n",
    "#   model=\"gpt-3.5-turbo-instruct\",\n",
    "#   prompt=f\"Summarize the following text: {input_text}\",\n",
    "#   max_tokens=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54513bf-bed4-40a5-b3e3-3050f1674c4f",
   "metadata": {
    "id": "f54513bf-bed4-40a5-b3e3-3050f1674c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists have made a groundbreaking discovery of water on Mars using a new spectroscopy technique.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687824c6-0c0e-4f15-bb5e-83903949bd0d",
   "metadata": {
    "id": "687824c6-0c0e-4f15-bb5e-83903949bd0d"
   },
   "source": [
    "Évidemment, cette approche est limitée à des textes dont la longueur ne dépasse pas celle du contexte du modèle (ici de 128k tokens uniquement). Pour résumer un texte plus long, il faudrait le découper en chunks, résumer ces chunks, les aggréger et ensuite les résumer (ou alors utiliser un modèle dont la fenêtre de contexte est plus longue...)\n",
    "\n",
    "Il y a de nombreuses façons de faire cela de facon un peu moins naïve, par exemple en déterminant les parties les plus importante du texte et en les résumant en priorité. On peut aussi utiliser des modèles un peu plus spécialisés pour ce genre de tâches.\n",
    "\n",
    "**Ici, nous allons résumer l'histoire de Cendrillon en implémentant cette stratégie de chunking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2e217-c209-446f-a1b1-a93aad441116",
   "metadata": {
    "id": "2fe2e217-c209-446f-a1b1-a93aad441116"
   },
   "source": [
    "**Exercice**: Charger le livre dans Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b99a9b-47e0-4f88-b1df-3f1d7403132e",
   "metadata": {
    "id": "72b99a9b-47e0-4f88-b1df-3f1d7403132e"
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/cinderella.txt\"  # Adjust as per your file path\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_to_summarize = file.read()\n",
    "\n",
    "# print(text_to_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sqGQ8vkyOeGQ",
   "metadata": {
    "id": "sqGQ8vkyOeGQ"
   },
   "source": [
    "**Exercice**: définir une fonction `get_chat_completion` pour envoyer un prompt à l'API d'OpenAI et récupérer le résultat sous forme de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9099849-c71e-4b8a-b61d-6cf19d41f7ad",
   "metadata": {
    "id": "f9099849-c71e-4b8a-b61d-6cf19d41f7ad"
   },
   "outputs": [],
   "source": [
    "from credentials.keys import OPENAI_API_KEY\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_chat_completion(messages, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2fef159-951f-4d95-801e-76a5c148ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the following text in a single sentence: {text_to_summarize}\"}\n",
    "    ]\n",
    "\n",
    "summary_from_openAi = get_chat_completion(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "416bb0a7-0890-4465-be12-bc040e9d57c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cinderella, a mistreated young girl forced into servitude by her overbearing stepmother and stepsisters, is magically transformed by her fairy godmother to attend a royal ball, where she captures the heart of the prince, ultimately revealing her true identity and leading to a happy ending for herself and her sisters.\n"
     ]
    }
   ],
   "source": [
    "print(summary_from_openAi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94140cd7-ee77-4c9a-9596-9f78a841ee7d",
   "metadata": {
    "id": "94140cd7-ee77-4c9a-9596-9f78a841ee7d"
   },
   "source": [
    "Ensuite, on définit les fonctions annexes pour tokéniser le texte, le découper en chunks dont le nombre de tokens ne dépasse pas une certaine valeur (pour gérer la limite sur la longueur de la fenêtre de contexte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92188d5f-2baf-4d73-850b-b31b384089e7",
   "metadata": {
    "id": "92188d5f-2baf-4d73-850b-b31b384089e7"
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    '''\n",
    "    Tokénise un texte en utilisant gpt-4o-mini\n",
    "    '''\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    return encoding.encode(text)\n",
    "\n",
    "\n",
    "def chunk_on_delimiter(\n",
    "        input_string: str,\n",
    "        max_tokens: int,\n",
    "        delimiter: str\n",
    "        ) -> list[str]:\n",
    "    '''\n",
    "    Chunk a text into smaller pieces based on a maximum token count and a delimiter.\n",
    "    '''\n",
    "    chunks = input_string.split(delimiter)\n",
    "    combined_chunks, _, dropped_chunk_count = combine_chunks(\n",
    "        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis_for_overflow=True\n",
    "    )\n",
    "    if dropped_chunk_count > 0:\n",
    "        print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n",
    "    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n",
    "    return combined_chunks\n",
    "\n",
    "\n",
    "def combine_chunks(\n",
    "        chunks: list[str],\n",
    "        max_tokens: int,\n",
    "        chunk_delimiter: str = \"\\n\\n\",\n",
    "        header: str = None,\n",
    "        add_ellipsis_for_overflow: bool = False,\n",
    "        ) -> tuple[list[str], list[int]]:\n",
    "    '''\n",
    "    Combine text chunks into larger blocks without exceeding a specified token count.\n",
    "    Return the combined text blocks, their original indices, and the count of chunks\n",
    "    dropped due to overflow.\n",
    "    '''\n",
    "    dropped_chunk_count = 0\n",
    "    output = []  # list to hold the final combined chunks\n",
    "    output_indices = []  # list to hold the indices of the final combined chunks\n",
    "    candidate = (\n",
    "        [] if header is None else [header]\n",
    "    )\n",
    "\n",
    "    candidate_indices = []\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "\n",
    "        chunk_with_header = [chunk] if header is None else [header, chunk]\n",
    "        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n",
    "            print(\"Warning: Chunk overflow\")\n",
    "            if (\n",
    "                    add_ellipsis_for_overflow\n",
    "                    and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n",
    "            ):\n",
    "                candidate.append(\"...\")\n",
    "                dropped_chunk_count += 1\n",
    "            continue\n",
    "\n",
    "        # estimate token count with the current chunk added\n",
    "        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n",
    "        # If the token count exceeds max_tokens, add the current candidate to output and start a new candidate\n",
    "        if extended_candidate_token_count > max_tokens:\n",
    "            output.append(chunk_delimiter.join(candidate))\n",
    "            output_indices.append(candidate_indices)\n",
    "            candidate = chunk_with_header  # re-initialize candidate\n",
    "            candidate_indices = [chunk_i]\n",
    "        # otherwise keep extending the candidate\n",
    "        else:\n",
    "            candidate.append(chunk)\n",
    "            candidate_indices.append(chunk_i)\n",
    "\n",
    "    # add the remaining candidate to output if it's not empty\n",
    "    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n",
    "        output.append(chunk_delimiter.join(candidate))\n",
    "        output_indices.append(candidate_indices)\n",
    "\n",
    "    return output, output_indices, dropped_chunk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8ef2e-80ec-4f4c-aa54-9b8e050bde23",
   "metadata": {
    "id": "a0b8ef2e-80ec-4f4c-aa54-9b8e050bde23"
   },
   "source": [
    "**Exercice** : écrire une fonction `summarize_text` qui résume un long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54a38509-7d45-45c8-ab62-c445805403c2",
   "metadata": {
    "id": "54a38509-7d45-45c8-ab62-c445805403c2"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def summarize(\n",
    "    text: str,\n",
    "    detail: float = 0,\n",
    "    model: str = 'gpt-4o-mini',\n",
    "    minimum_chunk_size: int = 2000,\n",
    "    chunk_delimiter: str = \".\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarizes a given text by splitting it into chunks, each of which is summarized individually.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be summarized.\n",
    "        detail (float, optional): A value between 0 and 1 indicating the desired level of detail in the summary.\n",
    "          0 leads to a higher level summary, and 1 results in a more detailed summary. Defaults to 0.\n",
    "        model (str, optional): The model to use for generating summaries. Defaults to 'gpt-3.5-turbo'.\n",
    "        minimum_chunk_size (Optional[int], optional): The minimum size for text chunks. Defaults to 500.\n",
    "        chunk_delimiter (str, optional): The delimiter used to split the text into chunks. Defaults to \".\".\n",
    "\n",
    "    Returns:\n",
    "    - str: The final compiled summary of the text.\n",
    "\n",
    "    The function first determines the number of chunks by interpolating between a minimum and a maximum chunk\n",
    "    count based on the `detail` parameter. It then splits the text into chunks and summarizes each chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # check detail is set correctly\n",
    "    assert 0 <= detail <= 1\n",
    "\n",
    "    # interpolate the number of chunks based to get specified level of detail\n",
    "    max_chunks = len(chunk_on_delimiter(text, minimum_chunk_size, chunk_delimiter))\n",
    "    min_chunks = 1\n",
    "    num_chunks = int(min_chunks + detail * (max_chunks - min_chunks))\n",
    "\n",
    "    # Adjust chunk_size based on interpolated number of chunks\n",
    "    document_length = len(tokenize(text))\n",
    "    chunk_size = max(minimum_chunk_size, document_length // num_chunks)\n",
    "    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n",
    "    print(f\"Splitting the text into {len(text_chunks)} chunks to be summarized.\")\n",
    "    print(f\"Chunk lengths are {[len(tokenize(x)) for x in text_chunks]}\")\n",
    "\n",
    "    # set system message\n",
    "    system_message_content = f\"You will be given different passages from a book one by one.\" + \\\n",
    "      f\"Provide a summary of the following text. When summarizing, directly dive into the narrative \" + \\\n",
    "      f\"or descriptions from the text without using introductory phrases like 'In this passage'.\" + \\\n",
    "      f\"Directly address the main events, characters, and themes, encapsulating the essence and \" + \\\n",
    "      f\"significant details from the text in a flowing narrative. The goal is to present a unified \" + \\\n",
    "      f\"view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\"\n",
    "\n",
    "    accumulated_summaries = []\n",
    "    for chunk in tqdm(text_chunks):\n",
    "        user_message_content = chunk\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content},\n",
    "            {\"role\": \"user\", \"content\": user_message_content}\n",
    "        ]\n",
    "        response = get_chat_completion(messages, model=model)\n",
    "        accumulated_summaries.append(response)\n",
    "\n",
    "    # Compile final summary from partial summaries\n",
    "    final_summary = '\\n\\n'.join(accumulated_summaries)\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "728f5fdb-87f4-4a8d-80a5-0a4ad11f8fa6",
   "metadata": {
    "id": "728f5fdb-87f4-4a8d-80a5-0a4ad11f8fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 1 chunks to be summarized.\n",
      "Chunk lengths are [6696]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.96s/it]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(text_to_summarize, detail=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07f2d52a-5012-4697-b894-9e42f66b1435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gentleman, having married a proud widow with two grown daughters, finds his gentle daughter from his first marriage, Cinderella, subjected to harsh treatment by her new stepfamily. The stepmother and her daughters, envious of Cinderella's goodness and beauty, force her into servitude, relegating her to the cinders and mocking her. Despite her mistreatment, Cinderella remains kind-hearted and patient, often dreaming of a better life.\n",
      "\n",
      "When the King announces a grand ball, the stepsisters are excited, and Cinderella is tasked with preparing their outfits. They belittle her, but she helps them nonetheless. On the night of the ball, Cinderella's fairy godmother appears, transforming a pumpkin into a coach, mice into horses, and giving Cinderella a stunning gown and glass slippers, warning her to leave before midnight.\n",
      "\n",
      "At the ball, Cinderella captivates the Prince, who is enchanted by her beauty. She dances with him, but as the clock strikes midnight, she flees, leaving behind one glass slipper. The Prince, determined to find the mysterious girl, searches the kingdom for the owner of the slipper.\n",
      "\n",
      "After several failed attempts with other ladies, Cinderella's stepsisters try to fit into the slipper but fail. Cinderella, encouraged by the Prime Minister, reveals herself as the beautiful girl from the ball when the slipper fits perfectly. Her fairy godmother enhances her appearance once more, and the stepsisters, realizing their mistake, beg for forgiveness. Cinderella forgives them and marries the Prince, inviting her sisters to live in the palace and marrying them off to noble lords, ensuring a happy ending for all.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19DMhdGLThv0",
   "metadata": {
    "id": "19DMhdGLThv0"
   },
   "source": [
    "On enregistre à présent le résumé sous forme de PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25YlRIQiTxAJ",
   "metadata": {
    "id": "25YlRIQiTxAJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (1.51.2)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (0.8.0)\n",
      "Collecting fpdf2\n",
      "  Downloading fpdf2-2.8.1-py2.py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain) (3.10.10)\n",
      "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain)\n",
      "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.135-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.12/site-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /opt/conda/lib/python3.12/site-packages (from fpdf2) (10.4.0)\n",
      "Collecting fonttools>=4.34.0 (from fpdf2)\n",
      "  Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fpdf2-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
      "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.135-py3-none-any.whl (295 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: tenacity, orjson, numpy, greenlet, fonttools, SQLAlchemy, requests-toolbelt, fpdf2, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed SQLAlchemy-2.0.35 fonttools-4.54.1 fpdf2-2.8.1 greenlet-3.1.1 langchain-0.3.3 langchain-core-0.3.10 langchain-text-splitters-0.3.0 langsmith-0.1.135 numpy-1.26.4 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai tiktoken fpdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa8b4e73-6ec8-44d1-97e9-cf96bf9c4d9e",
   "metadata": {
    "id": "aa8b4e73-6ec8-44d1-97e9-cf96bf9c4d9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2742/3246724405.py:6: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  self.set_font('Arial', 'B', 15)\n",
      "/tmp/ipykernel_2742/3246724405.py:10: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  self.cell(30, 10, 'Summary', 1, 0, 'C')\n",
      "/tmp/ipykernel_2742/3246724405.py:24: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "/tmp/ipykernel_2742/3246724405.py:18: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  self.set_font('Arial', 'I', 8)\n",
      "/tmp/ipykernel_2742/3246724405.py:20: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  self.cell(0, 10, 'Page %s' % self.page_no(), 0, 0, 'C')\n"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "   def header(self):\n",
    "       # Select Arial bold 15\n",
    "       self.set_font('Arial', 'B', 15)\n",
    "       # Move to the right\n",
    "       self.cell(80)\n",
    "       # Framed title\n",
    "       self.cell(30, 10, 'Summary', 1, 0, 'C')\n",
    "       # Line break\n",
    "       self.ln(20)\n",
    "\n",
    "   def footer(self):\n",
    "       # Go to 1.5 cm from bottom\n",
    "       self.set_y(-15)\n",
    "       # Select Arial italic 8\n",
    "       self.set_font('Arial', 'I', 8)\n",
    "       # Page number\n",
    "       self.cell(0, 10, 'Page %s' % self.page_no(), 0, 0, 'C')\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "summary_utf8 = summary.encode('latin-1', 'replace').decode('latin-1')\n",
    "pdf.multi_cell(0, 10, summary_utf8)\n",
    "pdf_output_path = \"./summary.pdf\"\n",
    "pdf.output(pdf_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0111c-88f2-4be0-bb92-abd567a7f60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "KVBxRp1vSJKy",
   "metadata": {
    "id": "KVBxRp1vSJKy"
   },
   "source": [
    "**Exercice (optionnel)**: adapter la fonction précédente pour résumer de manière récursive, c'est-à-dire en prenant en compte les résumés des chunks précédents pour chaque chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufxqz6MLSYp-",
   "metadata": {
    "id": "ufxqz6MLSYp-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5692ed8d-507b-4f92-bf9f-a489ef86c4f1",
   "metadata": {
    "id": "5692ed8d-507b-4f92-bf9f-a489ef86c4f1"
   },
   "source": [
    "On observe en pratique que cette approche ne fonctionne pas toujours très bien. On peut aller plus loin en utilisant des modèles dit encoder-decoder plus à même de résoudre ce genre de tâches (comme **BERT** ou **RoBERTA**).\n",
    "- https://www.width.ai/post/4-long-text-summarization-methods\n",
    "- https://medium.com/@pvsravanth/unlocking-the-power-of-text-summarization-with-large-language-models-llms-522372e7f9e0\n",
    "\n",
    "En explorant HuggingFace, on peut trouver plusieurs modèles qui sont censés être performants pour cette tâche :\n",
    "- **BERT_summary**: Une variante de BERT fine-tuned pour la summarization en particulier. https://huggingface.co/Shobhank-iiitdwd/BERT_summary/tree/main\n",
    "- **pegasus-multi_news** : https://huggingface.co/google/pegasus-multi_news PEGASUS = Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence. Développé par Google.\n",
    "- **longformer-base-4096** : https://huggingface.co/allenai/longformer-base-4096\n",
    "- **bigbird-roberta-base**: https://huggingface.co/google/bigbird-roberta-base by Google\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61c28c-dfdd-4c1b-9a00-35bdcd38c1aa",
   "metadata": {
    "id": "4a61c28c-dfdd-4c1b-9a00-35bdcd38c1aa"
   },
   "source": [
    "On peut commencer par essayer de voir si l'on peut facilement telecharger le modele \"Shobhank-iiitdwd/BERT_summary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d80f1-181c-4971-b9bc-a9ec9d7b47ea",
   "metadata": {},
   "source": [
    "### Trouble shoot on T5tokenizer \n",
    "\n",
    "If you have the below problem, `T5Tokenizer requires the SentencePiece library but it was not found in your environment.`\n",
    "\n",
    "Try to run the below code\n",
    "\n",
    "```shell\n",
    "# install the system dependencies\n",
    "sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
    "\n",
    "# install the python dependencies\n",
    "pip install \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c61d856-982a-415e-91e8-e2de8c8395a9",
   "metadata": {
    "id": "4c61d856-982a-415e-91e8-e2de8c8395a9"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained T5 model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShobhank-iiitdwd/BERT_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShobhank-iiitdwd/BERT_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:1637\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1637\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:1625\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1623\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Shobhank-iiitdwd/BERT_summary\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Shobhank-iiitdwd/BERT_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cbc7f-3135-46e6-bf1a-317e8d683a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc7836d5-7fa1-4cda-9d67-6aa246beea3c",
   "metadata": {
    "id": "cc7836d5-7fa1-4cda-9d67-6aa246beea3c"
   },
   "source": [
    "On tente maintenant le modele \"google/pegasus-multi_news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c598b670-66ed-4863-93ee-77f11cfbfa3f",
   "metadata": {
    "id": "c598b670-66ed-4863-93ee-77f11cfbfa3f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b481d425e842d983ad57aa253e8b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1385e15c50404b0cab2310ac70e7112d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc13d24cde94591a4484aca10f3d44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0e5386a37b4f7496148960d9dc502c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PegasusTokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type pegasus to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1343d90d55444989a17ccf3e0a49c7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.final_layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.12.layer.0.SelfAttention.k.weight', 'encoder.block.12.layer.0.SelfAttention.o.weight', 'encoder.block.12.layer.0.SelfAttention.q.weight', 'encoder.block.12.layer.0.SelfAttention.v.weight', 'encoder.block.12.layer.0.layer_norm.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight', 'encoder.block.12.layer.1.DenseReluDense.wo.weight', 'encoder.block.12.layer.1.layer_norm.weight', 'encoder.block.13.layer.0.SelfAttention.k.weight', 'encoder.block.13.layer.0.SelfAttention.o.weight', 'encoder.block.13.layer.0.SelfAttention.q.weight', 'encoder.block.13.layer.0.SelfAttention.v.weight', 'encoder.block.13.layer.0.layer_norm.weight', 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.13.layer.1.DenseReluDense.wo.weight', 'encoder.block.13.layer.1.layer_norm.weight', 'encoder.block.14.layer.0.SelfAttention.k.weight', 'encoder.block.14.layer.0.SelfAttention.o.weight', 'encoder.block.14.layer.0.SelfAttention.q.weight', 'encoder.block.14.layer.0.SelfAttention.v.weight', 'encoder.block.14.layer.0.layer_norm.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight', 'encoder.block.14.layer.1.DenseReluDense.wo.weight', 'encoder.block.14.layer.1.layer_norm.weight', 'encoder.block.15.layer.0.SelfAttention.k.weight', 'encoder.block.15.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.0.SelfAttention.q.weight', 'encoder.block.15.layer.0.SelfAttention.v.weight', 'encoder.block.15.layer.0.layer_norm.weight', 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.15.layer.1.DenseReluDense.wo.weight', 'encoder.block.15.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.final_layer_norm.weight', 'lm_head.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceee5b30450541c0afce1553030e8471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/pegasus-multi_news\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/pegasus-multi_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2405ec-1a95-4026-b4e4-7921e153ddf6",
   "metadata": {
    "id": "4a2405ec-1a95-4026-b4e4-7921e153ddf6"
   },
   "source": [
    "En fait, pour que cela fonction il faut télécharger le modèle directement via des fonctions plus spécialisées **PegasusTokenizer** ou **PegasusForConditionalGeneration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddc5ff8-a581-4bc7-bc5c-98a91f0deecc",
   "metadata": {
    "id": "1ddc5ff8-a581-4bc7-bc5c-98a91f0deecc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained PEGASUS tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-multi_news\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-multi_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86305d9a-31e5-480e-8cc5-1674ecf69b10",
   "metadata": {
    "id": "86305d9a-31e5-480e-8cc5-1674ecf69b10"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def summarize_text(\n",
    "        text: str,\n",
    "        max_length: int = 300,\n",
    "        min_length: int = 30,\n",
    "        num_beams: int = 4):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    -------\n",
    "        text: The input text to be summarized.\n",
    "        num_beams: Number of beams for beam search. Plus il y a de beam mieux c'est mais plus le cout computationnel explose.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(\"Summarize: \" + text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    print(f'Input shape: {inputs.shape}')\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs.to(device),\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=False\n",
    "    )\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf365e5c-b69e-41b4-8343-4ad3f9b84c4a",
   "metadata": {
    "id": "bf365e5c-b69e-41b4-8343-4ad3f9b84c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Input shape: torch.Size([1, 1024])\n",
      "– It's the story of a stepmother who forced her daughters to do the household work, and it's a tale that's been told before—but now it's getting new life as a children's book. In Cinderella, by Emma Donoghue, a young girl named Cinderella is forced to do the work of a stepmother who was a widow with two grown-up daughters. \"They scold me for that,\" she writes. \"If I do not run at once when the bell rings, they scold me for that. Yet they ring—both of them together sometimes—a minute after setting me to rake out a grate and sift the ashes. As for looking at myself in the glass, gladly would I do it if they allowed me one. But they have told me that if I had a glass I should only waste time in front of it. ... She kept these thoughts to herself, however, and suffered her ill-usage patiently, not daring to complain to her father, who would, moreover, have joined with the others in chiding her, for he was wholly under his wife's thumb. When she had done her work she used to creep away to the chimney-corner and seat herself among the cinders. ... They were wise in their way to deprive her of a looking-glass; for in truth, and in spite of her sorry rags, Cinderella was a hundred times more beautiful than they with all their magnificent dresses.\" Click for the full\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(text_to_summarize)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f908f4-e442-4e36-8a34-749120dee272",
   "metadata": {
    "id": "96f908f4-e442-4e36-8a34-749120dee272"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d35c0-0ec0-4950-8a97-e54d38200758",
   "metadata": {
    "id": "4b4d35c0-0ec0-4950-8a97-e54d38200758"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d734b4-4768-4133-ab48-97c71fe1d121",
   "metadata": {
    "id": "79d734b4-4768-4133-ab48-97c71fe1d121"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db07362-ab1e-474d-a5ad-5c5fe966472d",
   "metadata": {
    "id": "1db07362-ab1e-474d-a5ad-5c5fe966472d"
   },
   "source": [
    "Quelques remarques de ce TD\n",
    "- Il y a de nombreux modèles disponibles pour une tâche spécifique.\n",
    "- Commencer par tester si on arrive a y avoir accès facilement (pas d'erreur de package ou autre qui prendraient du temps à gérer).\n",
    "- Il faut tout de même faire un nombre d'expériences assez significatif.\n",
    "- Étant donné le livre que l'on a extrait, il vaudrait mieux bien nettoyer le texte pour éviter des ennuis inutiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d02da8-37de-4a08-90a4-082abc515322",
   "metadata": {
    "id": "98d02da8-37de-4a08-90a4-082abc515322"
   },
   "source": [
    "# 3) Named Entity Recognition NER\n",
    "\n",
    "C'est la tâche qui consiste à identifier et classifier les entités nommées (noms propres) dans des catégories prédéfinies telles que les noms de personnes, d'organisations, de lieux, de dates, de quantités, etc.\n",
    "\n",
    "Là encore, on peut utiliser du prompt pour résoudre cette tâche. Noter la encore que des modèles spécialisés peuvent etre plus performants et moins couteux pour ce genre de tâches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23475aa3-fe90-4bdf-99ee-d5592ebf2c9a",
   "metadata": {
    "id": "23475aa3-fe90-4bdf-99ee-d5592ebf2c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (1.51.2)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e384dc1f-16f6-41d2-9f03-3f1003296694",
   "metadata": {
    "id": "e384dc1f-16f6-41d2-9f03-3f1003296694"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken  # librairie de OpenAI\n",
    "\n",
    "from credentials.keys import OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Example text to analyze for named entities\n",
    "input_text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
    "\n",
    "# Construct a prompt to identify named entities\n",
    "prompt = f\"Identify the named entities in the following text. \" + \\\n",
    "    f\"You should return the entities in a list format, such as '['entity1', 'entity2', 'entity3']'\" + \\\n",
    "    f\"Here is the text: \\'{input_text}\\'.\"\n",
    "\n",
    "# Request completion from OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}  # Chat-based message format\n",
    "    ],\n",
    "    max_tokens=50  # Control response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee55e07-85eb-4718-848e-d1afac3febf7",
   "metadata": {
    "id": "3ee55e07-85eb-4718-848e-d1afac3febf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple Inc.', 'Cupertino', 'California']\n"
     ]
    }
   ],
   "source": [
    "# Process the response to extract named entities\n",
    "extracted_entities = response.choices[0].message.content\n",
    "\n",
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c7568-812a-4de2-8311-9eac647c7733",
   "metadata": {
    "id": "502c7568-812a-4de2-8311-9eac647c7733"
   },
   "source": [
    "Quels sont les modèles spécialisés pour faire de l'entity recognition ?\n",
    "\n",
    "En cherchant NER sur HuggingFace, on obtient par exemple :\n",
    "- bert-base-NER https://huggingface.co/dslim/bert-base-NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e18b40f-4b7d-4f0f-acc1-a5dfc592f046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422,
     "referenced_widgets": [
      "36277cf9b07f4d8c9ca0f28037bf2c19",
      "d6d019adc98a40a193055463f487e5b6",
      "29dc514a989e4b94adf5ca5a9cf9f075",
      "8ffc33c8367640d9b260dbba0c7ab68a",
      "1680ba054a894dee8188489e5f356253",
      "33050e54aae943ddac5b1699318e9322",
      "b25238e9c51d4545910e82daa6210bb1",
      "3bf60be6fab0459e8ed1b63932090611",
      "cb212653214e4221992fd034999c155b",
      "7a7937b8990841b7857ae71573c91a50",
      "4742af62f5ad435aa5c693935899cb63",
      "e954f0b1b05046de90c34da428bdbc60",
      "dc89deab7cb34cd89bfd52605c3c29bf",
      "9fad6745e7644a4cafe075c535b8b2e0",
      "96fd151b374546c9958cfe8ef9a3792d",
      "160e35f3fa89461eb27063a714edc32b",
      "4f0f3a394e9b48839047669fe9a621f3",
      "10ef24d7af9a4d0b87cbaf4a124fbc32",
      "3e15be00e7444d0dbd893abf3b7f582f",
      "b95c86eeaca7482dbdcd8484e85edc1b",
      "16acdbc124d849a491b91d9d26802fc7",
      "16a08a968de048ce975029ec1a0ec556",
      "a8d82fa2d54c4a4488c8845357bd39d8",
      "553a4e04df024eaea7c4c643628c74a1",
      "5a98c4049fe04b6aacf8a4f2777e4fd2",
      "53a4e60e650c4b6db40c0807eb9581fe",
      "ed19b3408a454669a46c8a0e8ac5a384",
      "29312d40d219459c94b500e38138f13b",
      "e99c95e261fe443995ebd6cd1617b89c",
      "661d7c92cadf41c8a18880a0f339c244",
      "baa9ef7bf33445cabda579951b806593",
      "fab667ffd17c47bc815b3c51f2733afc",
      "bee4a1fe79504819bf7ebe15ec7972d2",
      "a6a11ef680374abdb66b1e24ec0413d5",
      "0f6767fdba4b4727811b7565b1aca84d",
      "5787e1731c664d68bc2b51ceb4cc67ea",
      "34b53bcd05cc45938dc01bcca30c8b6d",
      "852966f64f5a4ef5971a07b131f29cdf",
      "93ee76fc32a04e2da6d6196d788c597d",
      "8bab2b96ab0d4222bdbab9f2936aadc0",
      "49f80ce1b2464dd28cf538b271e1d51e",
      "6dc7cc4130b745b08dff4035e0b1990b",
      "7054542451b34e31902609b5ab904278",
      "48e37642d3794dd1864982e3f82787c7",
      "e8c612e8df044aafb1232de87c83ecd0",
      "92170f9eeb6e44bab7a6f21aa08f6821",
      "19014a18e31a4bfbb75e1e723e91c4a9",
      "15ea31f5ccb64b719c9b4a919633e14c",
      "ce0936a7c375422390f3eee0a2a3bc50",
      "e1c8a76d6aca42e59c8f94ee8292629b",
      "a6fba52d70e24bd19d37aced1b0d0db4",
      "d18cda9e9a884082b22ba8d9005989bb",
      "26686e03aeb24b34b9d95a12c4078656",
      "95a18f8e2101469fa3ee9ff1564feb09",
      "0f2f539c4aab425db03482acbd18343b",
      "1259fe3fd9fa49808bd43ec6b47d11e0",
      "c6d4219591fd49a9a3813ae46391b994",
      "f4c234cfe00d418b90671516089adadf",
      "a888c9f8e49c486b8c664fb57a970d71",
      "a77b014fcb9f47c7aa9b54d8d1f514e3",
      "4d2501c4bd55475aaf66c096d8447724",
      "3b3b75a50c1e45368c90072e87819d17",
      "817a02813bfc4d27b9584592c1983f79",
      "60d132d4364a44e6988528163cd59123",
      "f181e353fb5744cb8d170e197c5bd56a",
      "bf1a323cde0444fbbd4a90d65bb1321e"
     ]
    },
    "executionInfo": {
     "elapsed": 48343,
     "status": "ok",
     "timestamp": 1728832016614,
     "user": {
      "displayName": "Alexandre Tuel",
      "userId": "09272815080792413028"
     },
     "user_tz": -120
    },
    "id": "2e18b40f-4b7d-4f0f-acc1-a5dfc592f046",
    "outputId": "6bd19480-1296-4c0a-f4ee-2f1356c53b00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7877e785-5b4b-41ae-8f51-bc64a40bd102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1728832091207,
     "user": {
      "displayName": "Alexandre Tuel",
      "userId": "09272815080792413028"
     },
     "user_tz": -120
    },
    "id": "7877e785-5b4b-41ae-8f51-bc64a40bd102",
    "outputId": "84f52d41-df7f-4dc5-d21e-a7af86fa490f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wolfgang', 'Berlin', 'France', 'Matthias', 'Italy']\n",
      "['B-PER', 'B-LOC', 'B-LOC', 'B-PER', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin but I often travel to France, although my friend Matthias lives in Italy.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print([d['word'] for d in ner_results])\n",
    "print([d['entity'] for d in ner_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baafc7f4-b338-4e99-9c99-aa3c444084cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'entity': 'B-LOC', 'score': np.float32(0.9997434), 'index': 23, 'word': 'Italy', 'start': 108, 'end': 113}, {'entity': 'B-PER', 'score': np.float32(0.99890935), 'index': 20, 'word': 'Matthias', 'start': 90, 'end': 98}, {'entity': 'B-LOC', 'score': np.float32(0.99979323), 'index': 15, 'word': 'France', 'start': 63, 'end': 69}, {'entity': 'B-LOC', 'score': np.float32(0.9997359), 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}, {'entity': 'B-PER', 'score': np.float32(0.9992913), 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}]\n"
     ]
    }
   ],
   "source": [
    "print(type(ner_results))\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gCtzm4hlaZ53",
   "metadata": {
    "id": "gCtzm4hlaZ53"
   },
   "source": [
    "**Exercice** : Anonymiser le texte grâce à la méthode de NER.\n",
    "\n",
    "*Indices* : Les indices de début/fin de chaque entité dans la chaîne de caractères de départ sont donnés par `ner_results[idx]['start']` et `ner_results[idx]['end']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rmb2flgtaZNn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1728832038383,
     "user": {
      "displayName": "Alexandre Tuel",
      "userId": "09272815080792413028"
     },
     "user_tz": -120
    },
    "id": "rmb2flgtaZNn",
    "outputId": "fe8555bd-2157-4bdd-9dd4-3e5a875058a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized Text: My name is PER and I live in LOC but I often travel to LOC, although my friend PER lives in LOC.\n"
     ]
    }
   ],
   "source": [
    "# Anonymize the text\n",
    "anonymized_text = example\n",
    "\n",
    "# Sort NER results in reverse order of their positions\n",
    "# This prevents messing up the indices while replacing\n",
    "ner_results = sorted(ner_results, key=lambda x: x['start'], reverse=True)\n",
    "\n",
    "# Replace entities with placeholders\n",
    "for entity in ner_results:\n",
    "    entity_label = entity['entity'].split('-')[-1]  # Get the entity type (e.g., PERSON, LOC, etc.)\n",
    "    start, end = entity['start'], entity['end']\n",
    "    anonymized_text = anonymized_text[:start] + entity_label + anonymized_text[end:]\n",
    "\n",
    "print(\"Anonymized Text:\", anonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pKvHn8HQbs5X",
   "metadata": {
    "id": "pKvHn8HQbs5X"
   },
   "source": [
    "## Optionnel : fine-tuning de NER sur un dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab679a67-112f-4cd6-8278-30dc0bd74654",
   "metadata": {
    "id": "ab679a67-112f-4cd6-8278-30dc0bd74654"
   },
   "source": [
    "Nous pouvons explorer le dataset **conll2003** qui est un dataset sur lesquel chaque les mots sont associes a leur part-of-speech tagging, leur NER. Pour l'instant nous n'allons pas encore evaluer les performances du model sur ce jeu de donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2443cdb-5aa1-46b2-9e47-9e262b72d63d",
   "metadata": {
    "id": "e2443cdb-5aa1-46b2-9e47-9e262b72d63d"
   },
   "outputs": [],
   "source": [
    "# On charge le dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_sentences(filepath: str):\n",
    "    '''\n",
    "    Load the CoNLL-2003 dataset.\n",
    "    '''\n",
    "    final, sentences = [], []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
    "                if len(sentences) > 0:\n",
    "                    final.append(sentences)\n",
    "                    sentences = []\n",
    "            else:\n",
    "                l = line.split(' ')\n",
    "                sentences.append((l[0], l[3].strip('\\n')))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7c90a59-f0b4-45c0-95dc-b4e5d5d3b8cb",
   "metadata": {
    "id": "a7c90a59-f0b4-45c0-95dc-b4e5d5d3b8cb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './conll003-englishversion/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./conll003-englishversion/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m \u001b[43mload_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m load_sentences(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m valid_samples \u001b[38;5;241m=\u001b[39m load_sentences(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mload_sentences\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mLoad the CoNLL-2003 dataset.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m final, sentences \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (line \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-DOCSTART- -X- -X- O\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './conll003-englishversion/train.txt'"
     ]
    }
   ],
   "source": [
    "base_path = './conll003-englishversion/'\n",
    "\n",
    "train_samples = load_sentences(base_path + 'train.txt')\n",
    "test_samples = load_sentences(base_path + 'test.txt')\n",
    "valid_samples = load_sentences(base_path + 'valid.txt')\n",
    "\n",
    "samples = train_samples + test_samples\n",
    "\n",
    "schema = ['_'] + sorted({tag for sentence in samples\n",
    "                             for _, tag in sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adccf76-d60d-4a3f-a7f2-7daa71f8ad66",
   "metadata": {
    "id": "4adccf76-d60d-4a3f-a7f2-7daa71f8ad66",
    "outputId": "0fdb22bc-0a5d-481c-8cd3-2e3cf8b50ea8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'I-LOC',\n",
       " 'I-MISC',\n",
       " 'I-ORG',\n",
       " 'I-PER',\n",
       " 'O']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91cd14-0160-46be-8133-d24e96e97bd2",
   "metadata": {
    "id": "8f91cd14-0160-46be-8133-d24e96e97bd2",
    "outputId": "08171176-0d18-4ad3-d563-ea0c5fc9d3a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, TFAutoModelForTokenClassification\n",
    "\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96e4d3-1b1a-4a7f-905b-047267e91077",
   "metadata": {
    "id": "dc96e4d3-1b1a-4a7f-905b-047267e91077",
    "outputId": "d0cd5fba-50ae-40bb-ef08-2e203e9e2f78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14041it [00:05, 2702.84it/s]\n",
      "3452it [00:01, 2957.40it/s]\n",
      "3249it [00:01, 2517.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_sample(sample):\n",
    "    seq = [\n",
    "               (subtoken, tag)\n",
    "               for token, tag in sample\n",
    "               for subtoken in tokenizer(token)['input_ids'][1:-1]\n",
    "           ]\n",
    "    return [(3, 'O')] + seq + [(4, 'O')]\n",
    "\n",
    "def preprocess(samples):\n",
    "    tag_index = {tag: i for i, tag in enumerate(schema)}\n",
    "    tokenized_samples = list(tqdm(map(tokenize_sample, samples)))\n",
    "    max_len = max(map(len, tokenized_samples))\n",
    "    X = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "    y = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "    for i, sentence in enumerate(tokenized_samples):\n",
    "        for j, (subtoken_id, tag) in enumerate(sentence):\n",
    "            X[i, j] = subtoken_id\n",
    "            y[i,j] = tag_index[tag]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = preprocess(train_samples)\n",
    "X_test, y_test = preprocess(test_samples)\n",
    "X_valid, y_valid = preprocess(valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d591919-dbb5-4ef1-b46d-0ed299b5e995",
   "metadata": {
    "id": "1d591919-dbb5-4ef1-b46d-0ed299b5e995"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.000001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "history = model.fit(\n",
    "    tf.constant(X_train), tf.constant(y_train),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2aea29-7736-46f1-b4dd-3a168b2c0732",
   "metadata": {
    "id": "cc2aea29-7736-46f1-b4dd-3a168b2c0732"
   },
   "outputs": [],
   "source": [
    "[loss, accuracy] = model.evaluate(X_valid, y_valid)\n",
    "print(\"Loss:%1.3f, Accuracy:%1.3f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000b108-1ad7-4278-8ef9-15dcd4140929",
   "metadata": {
    "id": "8000b108-1ad7-4278-8ef9-15dcd4140929"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f2f539c4aab425db03482acbd18343b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f6767fdba4b4727811b7565b1aca84d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ee76fc32a04e2da6d6196d788c597d",
      "placeholder": "​",
      "style": "IPY_MODEL_8bab2b96ab0d4222bdbab9f2936aadc0",
      "value": "added_tokens.json: 100%"
     }
    },
    "10ef24d7af9a4d0b87cbaf4a124fbc32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1259fe3fd9fa49808bd43ec6b47d11e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6d4219591fd49a9a3813ae46391b994",
       "IPY_MODEL_f4c234cfe00d418b90671516089adadf",
       "IPY_MODEL_a888c9f8e49c486b8c664fb57a970d71"
      ],
      "layout": "IPY_MODEL_a77b014fcb9f47c7aa9b54d8d1f514e3"
     }
    },
    "15ea31f5ccb64b719c9b4a919633e14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95a18f8e2101469fa3ee9ff1564feb09",
      "placeholder": "​",
      "style": "IPY_MODEL_0f2f539c4aab425db03482acbd18343b",
      "value": " 112/112 [00:00&lt;00:00, 976B/s]"
     }
    },
    "160e35f3fa89461eb27063a714edc32b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1680ba054a894dee8188489e5f356253": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16a08a968de048ce975029ec1a0ec556": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16acdbc124d849a491b91d9d26802fc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19014a18e31a4bfbb75e1e723e91c4a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d18cda9e9a884082b22ba8d9005989bb",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26686e03aeb24b34b9d95a12c4078656",
      "value": 112
     }
    },
    "26686e03aeb24b34b9d95a12c4078656": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29312d40d219459c94b500e38138f13b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29dc514a989e4b94adf5ca5a9cf9f075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bf60be6fab0459e8ed1b63932090611",
      "max": 59,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb212653214e4221992fd034999c155b",
      "value": 59
     }
    },
    "33050e54aae943ddac5b1699318e9322": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34b53bcd05cc45938dc01bcca30c8b6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7054542451b34e31902609b5ab904278",
      "placeholder": "​",
      "style": "IPY_MODEL_48e37642d3794dd1864982e3f82787c7",
      "value": " 2.00/2.00 [00:00&lt;00:00, 15.9B/s]"
     }
    },
    "36277cf9b07f4d8c9ca0f28037bf2c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6d019adc98a40a193055463f487e5b6",
       "IPY_MODEL_29dc514a989e4b94adf5ca5a9cf9f075",
       "IPY_MODEL_8ffc33c8367640d9b260dbba0c7ab68a"
      ],
      "layout": "IPY_MODEL_1680ba054a894dee8188489e5f356253"
     }
    },
    "3b3b75a50c1e45368c90072e87819d17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3bf60be6fab0459e8ed1b63932090611": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e15be00e7444d0dbd893abf3b7f582f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4742af62f5ad435aa5c693935899cb63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48e37642d3794dd1864982e3f82787c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49f80ce1b2464dd28cf538b271e1d51e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d2501c4bd55475aaf66c096d8447724": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f0f3a394e9b48839047669fe9a621f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53a4e60e650c4b6db40c0807eb9581fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fab667ffd17c47bc815b3c51f2733afc",
      "placeholder": "​",
      "style": "IPY_MODEL_bee4a1fe79504819bf7ebe15ec7972d2",
      "value": " 213k/213k [00:00&lt;00:00, 1.38MB/s]"
     }
    },
    "553a4e04df024eaea7c4c643628c74a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29312d40d219459c94b500e38138f13b",
      "placeholder": "​",
      "style": "IPY_MODEL_e99c95e261fe443995ebd6cd1617b89c",
      "value": "vocab.txt: 100%"
     }
    },
    "5787e1731c664d68bc2b51ceb4cc67ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49f80ce1b2464dd28cf538b271e1d51e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dc7cc4130b745b08dff4035e0b1990b",
      "value": 2
     }
    },
    "5a98c4049fe04b6aacf8a4f2777e4fd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_661d7c92cadf41c8a18880a0f339c244",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_baa9ef7bf33445cabda579951b806593",
      "value": 213450
     }
    },
    "60d132d4364a44e6988528163cd59123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "661d7c92cadf41c8a18880a0f339c244": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dc7cc4130b745b08dff4035e0b1990b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7054542451b34e31902609b5ab904278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a7937b8990841b7857ae71573c91a50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "817a02813bfc4d27b9584592c1983f79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "852966f64f5a4ef5971a07b131f29cdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bab2b96ab0d4222bdbab9f2936aadc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ffc33c8367640d9b260dbba0c7ab68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a7937b8990841b7857ae71573c91a50",
      "placeholder": "​",
      "style": "IPY_MODEL_4742af62f5ad435aa5c693935899cb63",
      "value": " 59.0/59.0 [00:00&lt;00:00, 553B/s]"
     }
    },
    "92170f9eeb6e44bab7a6f21aa08f6821": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1c8a76d6aca42e59c8f94ee8292629b",
      "placeholder": "​",
      "style": "IPY_MODEL_a6fba52d70e24bd19d37aced1b0d0db4",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "93ee76fc32a04e2da6d6196d788c597d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95a18f8e2101469fa3ee9ff1564feb09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96fd151b374546c9958cfe8ef9a3792d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16acdbc124d849a491b91d9d26802fc7",
      "placeholder": "​",
      "style": "IPY_MODEL_16a08a968de048ce975029ec1a0ec556",
      "value": " 829/829 [00:00&lt;00:00, 16.9kB/s]"
     }
    },
    "9fad6745e7644a4cafe075c535b8b2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e15be00e7444d0dbd893abf3b7f582f",
      "max": 829,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b95c86eeaca7482dbdcd8484e85edc1b",
      "value": 829
     }
    },
    "a6a11ef680374abdb66b1e24ec0413d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f6767fdba4b4727811b7565b1aca84d",
       "IPY_MODEL_5787e1731c664d68bc2b51ceb4cc67ea",
       "IPY_MODEL_34b53bcd05cc45938dc01bcca30c8b6d"
      ],
      "layout": "IPY_MODEL_852966f64f5a4ef5971a07b131f29cdf"
     }
    },
    "a6fba52d70e24bd19d37aced1b0d0db4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a77b014fcb9f47c7aa9b54d8d1f514e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a888c9f8e49c486b8c664fb57a970d71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f181e353fb5744cb8d170e197c5bd56a",
      "placeholder": "​",
      "style": "IPY_MODEL_bf1a323cde0444fbbd4a90d65bb1321e",
      "value": " 433M/433M [00:05&lt;00:00, 88.0MB/s]"
     }
    },
    "a8d82fa2d54c4a4488c8845357bd39d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_553a4e04df024eaea7c4c643628c74a1",
       "IPY_MODEL_5a98c4049fe04b6aacf8a4f2777e4fd2",
       "IPY_MODEL_53a4e60e650c4b6db40c0807eb9581fe"
      ],
      "layout": "IPY_MODEL_ed19b3408a454669a46c8a0e8ac5a384"
     }
    },
    "b25238e9c51d4545910e82daa6210bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b95c86eeaca7482dbdcd8484e85edc1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "baa9ef7bf33445cabda579951b806593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bee4a1fe79504819bf7ebe15ec7972d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf1a323cde0444fbbd4a90d65bb1321e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6d4219591fd49a9a3813ae46391b994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d2501c4bd55475aaf66c096d8447724",
      "placeholder": "​",
      "style": "IPY_MODEL_3b3b75a50c1e45368c90072e87819d17",
      "value": "model.safetensors: 100%"
     }
    },
    "cb212653214e4221992fd034999c155b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce0936a7c375422390f3eee0a2a3bc50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d18cda9e9a884082b22ba8d9005989bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6d019adc98a40a193055463f487e5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33050e54aae943ddac5b1699318e9322",
      "placeholder": "​",
      "style": "IPY_MODEL_b25238e9c51d4545910e82daa6210bb1",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "dc89deab7cb34cd89bfd52605c3c29bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f0f3a394e9b48839047669fe9a621f3",
      "placeholder": "​",
      "style": "IPY_MODEL_10ef24d7af9a4d0b87cbaf4a124fbc32",
      "value": "config.json: 100%"
     }
    },
    "e1c8a76d6aca42e59c8f94ee8292629b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8c612e8df044aafb1232de87c83ecd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92170f9eeb6e44bab7a6f21aa08f6821",
       "IPY_MODEL_19014a18e31a4bfbb75e1e723e91c4a9",
       "IPY_MODEL_15ea31f5ccb64b719c9b4a919633e14c"
      ],
      "layout": "IPY_MODEL_ce0936a7c375422390f3eee0a2a3bc50"
     }
    },
    "e954f0b1b05046de90c34da428bdbc60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc89deab7cb34cd89bfd52605c3c29bf",
       "IPY_MODEL_9fad6745e7644a4cafe075c535b8b2e0",
       "IPY_MODEL_96fd151b374546c9958cfe8ef9a3792d"
      ],
      "layout": "IPY_MODEL_160e35f3fa89461eb27063a714edc32b"
     }
    },
    "e99c95e261fe443995ebd6cd1617b89c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed19b3408a454669a46c8a0e8ac5a384": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f181e353fb5744cb8d170e197c5bd56a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4c234cfe00d418b90671516089adadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_817a02813bfc4d27b9584592c1983f79",
      "max": 433292294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_60d132d4364a44e6988528163cd59123",
      "value": 433292294
     }
    },
    "fab667ffd17c47bc815b3c51f2733afc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
